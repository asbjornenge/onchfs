{"version":3,"sources":["../src/files/index.ts","../src/config.ts","../src/utils/index.ts","../src/utils/keccak.ts","../src/utils/string.ts","../src/utils/uint8.ts","../src/files/chunks.ts","../src/files/file.ts","../src/metadata/encode.ts","../src/metadata/validation.ts","../src/metadata/shared.ts","../src/files/directory.ts","../src/files/prepare.ts","../src/inscriptions/index.ts","../src/inscriptions/estimate.ts","../src/inscriptions/prepare.ts","../src/metadata/index.ts","../src/metadata/decode.ts","../src/resolver/index.ts","../src/resolver/errors.ts","../src/types/resolver.ts","../src/types/uri.ts","../src/uri/parse.ts","../src/resolver/proxy.ts","../src/uri/index.ts","../src/index.ts"],"sourcesContent":["import { chunkBytes } from \"./chunks\"\nimport {\n  buildDirectoryGraph,\n  computeDirectoryInode,\n  encodeFilename,\n} from \"./directory\"\n\nexport { prepare } from \"./prepare\"\nexport const utils = {\n  chunkBytes,\n  buildDirectoryGraph,\n  computeDirectoryInode,\n  encodeFilename,\n}\n","// an identifying byte is used when hasing files & directories, in order to\n// prevent eventual collisions at a high level\nexport const INODE_BYTE_IDENTIFIER = {\n  FILE: new Uint8Array([1]),\n  DIRECTORY: new Uint8Array([0]),\n}\n\n// sort of a mgic value, as it's impossible to have a single number to rule\n// them all; applications would have to pick the right chunk size here as to\n// improve storage being shared as much as possible depending on the use cases\nexport const DEFAULT_CHUNK_SIZE = 16384\n\n// TODO: insert true values here.\n// A naive map of the \"official\" onchfs Smart Contracts.\nexport const DEFAULT_CONTRACTS: Record<string, string> = {\n  \"tezos:mainnet\": \"KT1WvzYHCNBvDSdwafTHv7nJ1dWmZ8GCYuuC\",\n  \"tezos:ghostnet\": \"KT1XZ2FyRNtzYCBoy18Rp7R9oejvFSPqkBoy\",\n  \"ethereum:1\": \"b0e58801d1b4d69179b7bc23fe54a37cee999b09\",\n  \"ethereum:5\": \"fcfdfa971803e1cc201f80d8e74de71fddea6551\",\n}\n","import { keccak } from \"./keccak\"\nimport { hexStringToBytes } from \"./string\"\nimport {\n  BytesCopiedFrom,\n  areUint8ArrayEqual,\n  compareUint8Arrays,\n  concatUint8Arrays,\n} from \"./uint8\"\n\nexport {\n  hexStringToBytes,\n  keccak,\n  BytesCopiedFrom,\n  concatUint8Arrays,\n  compareUint8Arrays,\n  areUint8ArrayEqual,\n}\n","import { keccak256 } from \"js-sha3\"\n\n/**\n * Hashes some bytes with keccak256. Simple typed wrapper to ease implementation\n * @param bytes Bytes to hash\n */\nexport function keccak(bytes: Uint8Array | string): Uint8Array {\n  return new Uint8Array(keccak256.digest(bytes))\n}\n","/**\n * Converts a hexadecimal string in a list of bytes, considering each pair\n * of characters in the string represents a byte. Will throw if the format\n * of the string is incorrect.\n * @param hex A hexadecimal string\n * @returns The bytes decoded from the hexadecimal string\n */\nexport function hexStringToBytes(hex: string): Uint8Array {\n  const reg = new RegExp(\"^(?:[a-fA-F0-9]{2})*$\")\n  if (!reg.exec(hex)) {\n    throw new Error(\n      `Cannot decode an hexadecimal string because its pattern is invalid\\nExpected: ${reg.toString()}\\nGot ${hex.slice(\n        0,\n        80\n      )}${hex.length > 80 ? \"...\" : \"\"}`\n    )\n  }\n  const bytes = new Uint8Array(hex.length / 2)\n  for (let i = 0; i < hex.length / 2; i++) {\n    bytes[i] = parseInt(hex.slice(i * 2, i * 2 + 2), 16)\n  }\n  return bytes\n}\n","/**\n * Intanciates a new Uint8Array in which the requested bytes from the source\n * are copied into. Inspired by nodejs Bytes.copyBytesFrom()\n * @param source The source to copy from\n * @param offset Offset in the source\n * @param length Number of bytes to copy after the offset. If undefined (def),\n * will copy everything after the offset.\n * @returns A new Uint8Array\n */\nexport function BytesCopiedFrom(\n  source: Uint8Array,\n  offset = 0,\n  length?: number\n) {\n  length = typeof length === \"undefined\" ? source.byteLength - offset : length\n  const out = new Uint8Array(length)\n  for (let i = 0; i < length; i++) {\n    out[i] = source[i + offset]\n  }\n  return out\n}\n\n/**\n * Instanciates a new Uint8Array and concatenates the given Uint8Arrays\n * together in the newly instanciated array.\n * @param arrays The Uint8Arrays to concatenate together\n * @returns A new Uint8Array\n */\nexport function concatUint8Arrays(...arrays: Uint8Array[]): Uint8Array {\n  const L = arrays.reduce((acc, arr) => arr.length + acc, 0)\n  const out = new Uint8Array(L)\n  let offset = 0\n  for (const arr of arrays) {\n    out.set(arr, offset)\n    offset += arr.length\n  }\n  return out\n}\n\n/**\n * Na√Øve Uint8Arrays comparaison for sorting. Loops through the bytes of array A\n * and compare their value against bytes of array B at the same index.\n * @param a First Uint8Array to compare\n * @param b Second Uint8Array to compare\n * @returns -1 if a < b, otherwise 1\n */\nexport function compareUint8Arrays(a: Uint8Array, b: Uint8Array): number {\n  // negative if a is less than b\n  for (let i = 0; i < a.length; i++) {\n    if (a[i] < b[i]) return -1\n    if (a[i] > b[i]) return 1\n  }\n  return 1\n}\n\n/**\n * Equality comparaison between 2 Uint8Arrays. Arrays are equal if they have the\n * same length and if all their components are equal to their counterpart\n * components at the same index.\n * @param a\n * @param b\n * @returns true if equal, false otherwise\n */\nexport function areUint8ArrayEqual(a: Uint8Array, b: Uint8Array): boolean {\n  if (a.length !== b.length) return false\n  for (let i = 0; i < a.length; i++) {\n    if (a[i] !== b[i]) return false\n  }\n  return true\n}\n","import { DEFAULT_CHUNK_SIZE } from \"@/config\"\nimport { FileChunk } from \"@/types/files\"\nimport { BytesCopiedFrom, keccak } from \"@/utils\"\n\n/**\n * Splits the content of a file into multiple chunks of the same size (except\n * if the remaining bytes of the last chunk don't cover a full chunk, in which\n * case a smaller chunk upload will be required). Chunks are also hashed, as\n * such this function returns tuples of (chunk, chunkHash).\n * @param content Raw byte content of the file\n * @param chunkSize Size of the chunks, it's recommend to pick the highest\n * possible chunk size for the targetted blockchain as to optimise the number\n * of write operations which will be performed. Depending on the use-case there\n * might be a need to create smaller chunks to allow for redundancy of similar\n * chunks uploaded to kick-in, resulting in write optimisations. As a reminder,\n * 32 bytes will be used to address a chunk in the store, as such every chunk\n * to be stored requires 32 bytes of extra storage.\n * @returns a list of chunks which can be uploaded to reconstruct the file\n */\nexport function chunkBytes(\n  content: Uint8Array,\n  chunkSize: number = DEFAULT_CHUNK_SIZE\n): FileChunk[] {\n  if (chunkSize == 0) {\n    throw new Error(`invalid chunk size, must be positive integer`)\n  }\n  const L = content.length\n  const nb = Math.ceil(L / chunkSize)\n  const chunks: FileChunk[] = []\n  let chunk: Uint8Array\n  for (let i = 0; i < nb; i++) {\n    chunk = BytesCopiedFrom(\n      content,\n      i * chunkSize,\n      Math.min(chunkSize, L - i * chunkSize)\n    )\n    chunks.push({\n      bytes: chunk,\n      hash: keccak(chunk),\n    })\n  }\n  return chunks\n}\n","import { gzip } from \"pako\"\nimport { DEFAULT_CHUNK_SIZE, INODE_BYTE_IDENTIFIER } from \"@/config\"\nimport { lookup as lookupMime } from \"mime-types\"\nimport { chunkBytes } from \"./chunks\"\nimport { concatUint8Arrays, keccak } from \"@/utils\"\nimport { FileMetadataEntries } from \"@/types/metadata\"\nimport { encodeMetadata } from \"@/metadata/encode\"\nimport { FileInode, IFile } from \"@/types/files\"\n// import { fileTypeFromBuffer } from \"file-type\"\n\n/**\n * Computes all the necessary data for the inscription of the file on-chain.\n * Performs the following tasks in order:\n *  - infer/detect the mime-type from the filename/content\n *  - compress the content in gzip using zopfli, but only use the compressed\n *    bytes if they are smaller in size than the raw content\n *  - build the metadata object from previous steps, and encode the metadata in\n *    the format supported by the blockchain\n *  - chunk the content of the file\n *  - compute the CID of the file based on its content & its metadata\n * @param name The filename, will only be used to infer the Mime Type (a magic number cannot be used for text files so this is the privileged method)\n * @param content Byte content of the file, as a Buffer\n * @param chunkSize Max number of bytes for chunking the file content\n * @returns A file node object with all the data necessary for its insertion\n */\nexport async function prepareFile(\n  file: IFile,\n  chunkSize: number = DEFAULT_CHUNK_SIZE\n): Promise<FileInode> {\n  const { path: name, content } = file\n  let metadata: FileMetadataEntries = {}\n  let insertionBytes = content\n  // we use file extension to get mime type\n  let mime = lookupMime(name)\n  // if no mime type can be mapped from filename, use magic number\n  if (!mime) {\n    // const magicMime = await fileTypeFromBuffer(content)\n    // if (magicMime) {\n    //   metadata[\"Content-Type\"] = magicMime.mime\n    // }\n    // if still no mime, we simply do not set the Content-Type in the metadata,\n    // and let the browser handle it.\n    // We could set it to \"application/octet-stream\" as RFC2046 states, however\n    // we'd be storing this whole string on-chain for something that's probably\n    // going to be inferred as such in any case;\n  } else {\n    metadata[\"Content-Type\"] = mime\n  }\n\n  // compress into gzip using node zopfli, only keep if better\n  const compressed = gzip(content)\n  if (compressed.byteLength < insertionBytes.byteLength) {\n    insertionBytes = compressed\n    metadata[\"Content-Encoding\"] = \"gzip\"\n  }\n\n  // chunk the file\n  const chunks = chunkBytes(insertionBytes, chunkSize)\n  // encode the metadata\n  const metadataEncoded = encodeMetadata(metadata)\n  // compute the file unique identifier, following the onchfs specifications:\n  // keccak( 0x01 , keccak( content ), keccak( metadata ) )\n  const contentHash = keccak(insertionBytes)\n  const metadataHash = keccak(metadataEncoded)\n  const cid = keccak(\n    concatUint8Arrays(INODE_BYTE_IDENTIFIER.FILE, contentHash, metadataHash)\n  )\n\n  return {\n    type: \"file\",\n    cid,\n    chunks,\n    metadata: metadataEncoded,\n  }\n}\n","import hpack from \"hpack.js\"\nimport { validateMetadataValue } from \"./validation\"\nimport { fieldHpackStaticTableIndex } from \"./shared\"\nimport { FileMetadataEntries } from \"@/types/metadata\"\n\n/**\n * Encodes the metadata of a file following the specifications provided by the\n * onchfs. Each entry is prefixed by 2 bytes encoding the entry type, followed\n * by 7-bit ASCII encoded characters for the string-value associated.\n * The metadata entries are sorted by their 2 bytes identifier.\n * @param metadata The object metadata of a file\n * @returns An array of buffers, each entry representing one metadata property\n */\nexport function encodeMetadata(metadata: FileMetadataEntries): Uint8Array {\n  const comp = hpack.compressor.create({ table: { size: 256 } })\n  let headers: any[] = []\n  let name: string, value: string\n  for (const entry in metadata) {\n    name = entry.toLowerCase()\n    value = metadata[entry]\n    try {\n      validateMetadataValue(value)\n    } catch (err: any) {\n      throw new Error(\n        `Error when validating the metadata field \"${entry}\": ${err.message}`\n      )\n    }\n    headers.push({\n      name,\n      value,\n    })\n  }\n\n  // sort the headers based on their indices in the HPACK static table\n  headers = headers.sort((a, b) => {\n    const iA = fieldHpackStaticTableIndex(a.name)\n    const iB = fieldHpackStaticTableIndex(b.name)\n    if (iA === null && iB !== null) return -1\n    if (iB === null && iA !== null) return 1\n    if (iA === null && iB === null) return 0\n    return iB - iA\n  })\n  comp.write(headers)\n  // ensures proper Uint8array typing at the end of the call\n  return new Uint8Array(comp.read())\n}\n","// a list of the forbidden characters in the metadata\n// todo: point to where I found this in http specs\nexport const FORBIDDEN_METADATA_CHARS = [\n  0, // NUL character\n]\n\n/**\n * Validate a metadata field value to check if if follows https contrasts.\n * todo: should be refined to properly implement the HTTP spec, right now just\n *       NUL is verified\n * @param value The metadata field value\n */\nexport function validateMetadataValue(value: string): void {\n  for (let i = 0; i < value.length; i++) {\n    if (FORBIDDEN_METADATA_CHARS.includes(value.charCodeAt(i))) {\n      throw new Error(\n        `contains invalid character (code: ${value.charCodeAt(\n          i\n        )}) at position ${i}`\n      )\n    }\n  }\n}\n","import hpack from \"hpack.js\"\n\n/**\n * Given a field name, outputs its index in the HPACK static table.\n * @param name Name of the field to check in the static table\n */\nexport function fieldHpackStaticTableIndex(name: string): number | null {\n  const elem = hpack[\"static-table\"].table.find(\n    row => row.name === name.toLowerCase()\n  )\n  if (!elem) return null\n  return hpack[\"static-table\"].table.indexOf(elem)\n}\n","import { DEFAULT_CHUNK_SIZE, INODE_BYTE_IDENTIFIER } from \"../config\"\nimport { prepareFile } from \"@/files/file\"\nimport {\n  DirectoryInode,\n  IFile,\n  INode,\n  PrepareDirectoryDir,\n  PrepareDirectoryFile,\n  PrepareDirectoryNode,\n} from \"@/types/files\"\nimport { concatUint8Arrays, keccak } from \"@/utils\"\n\n/**\n * Encodes the filename in 7-bit ASCII, where UTF-8 characters are escaped. Will\n * also escape any character that are not supported in the URI specification, as\n * these will be fetched using a similar pattern by browsers. The native\n * `encodeURIComponent()` method will be used for such a purpose.\n * @param name Filename to encode\n * @returns Filename encoded in 7-bit ASCII\n */\nexport function encodeFilename(name: string): string {\n  return encodeURIComponent(name)\n}\n\n/**\n * Computed the different component of a directory inode based on the\n * preparation object.\n * @param dir A directory being prepared\n * @returns A directory inode, from which insertions can be derived\n */\nexport function computeDirectoryInode(\n  dir: PrepareDirectoryDir\n): DirectoryInode {\n  const acc: Uint8Array[] = []\n  const filenames = Object.keys(dir.files).sort()\n  const dirFiles: Record<string, INode> = {}\n  for (const filename of filenames) {\n    const inode = dir.files[filename].inode!\n    dirFiles[filename] = inode\n    // push filename hashed\n    acc.unshift(keccak(filename))\n    // push target inode cid\n    acc.unshift(inode.cid)\n  }\n  // add indentifying byte at the beginning\n  acc.unshift(INODE_BYTE_IDENTIFIER.DIRECTORY)\n  return {\n    type: \"directory\",\n    cid: keccak(concatUint8Arrays(...acc)),\n    files: dirFiles,\n  }\n}\n\n/**\n * Builds a graph from a list of files (relative path from the directory root,\n * content) in a folder structure as it's going to be inscribed on the file\n * system.\n * @param files A list of the files (& their content), where paths are specified with separating \"/\"\n * @returns A tuple of (graph, leaves), where graph is a structure ready to be\n * parsed for insertion & leaves the leaves of the graph, entry points for\n * parsing the graph in reverse.\n */\nexport function buildDirectoryGraph(\n  files: IFile[]\n): [PrepareDirectoryDir, PrepareDirectoryFile[]] {\n  let graph: PrepareDirectoryDir = {\n    type: \"directory\",\n    files: {},\n    parent: null,\n  }\n  const leaves: PrepareDirectoryFile[] = []\n\n  for (const file of files) {\n    let active = graph,\n      part: string = \"\"\n    const formattedPath = file.path.startsWith(\"./\")\n      ? file.path.slice(2)\n      : file.path\n    // note: the filenames get encoded here, as the onchfs spec defines\n    // filenames need to be inserted in ASCII 7-bit with special characters\n    // escape-encoded\n    const parts = formattedPath.split(\"/\").map(part => encodeFilename(part))\n    for (let i = 0; i < parts.length; i++) {\n      part = parts[i]\n      // if name is empty, we throw\n      if (part.length === 0) {\n        throw new Error(\n          `The file ${file.path} contains an invalid part, there must be at least 1 character for each part.`\n        )\n      }\n      // if it's the last part, store it as a file\n      if (i === parts.length - 1) {\n        // if the leaf already exists, we throw an error: there cannot be 2\n        // nodes identified by the same path\n        if (active.files.hasOwnProperty(part)) {\n          throw new Error(\n            `The file at path ${file.path} is colliding with another path in the directory. There mush be a single path pointing to a file.`\n          )\n        }\n        const nLeave: PrepareDirectoryFile = {\n          type: \"file\",\n          content: file.content,\n          name: part,\n          parent: active,\n        }\n        active.files[part] = nLeave\n        leaves.push(nLeave)\n      }\n      // it's a directory, so we need to navigate to it or create a new one\n      else {\n        if (active.files.hasOwnProperty(part)) {\n          active = active.files[part] as any\n        } else {\n          const nDir: PrepareDirectoryDir = {\n            type: \"directory\",\n            files: {},\n            parent: active,\n          }\n          active.files[part] = nDir\n          active = nDir\n        }\n      }\n    }\n  }\n\n  return [graph, leaves]\n}\n\n/**\n * Given a list of files, will create an inverted tree of the directory\n * structure with the main directory as its root. Each file will be chunked in\n * preparation for the insertion. The whole structure will be ready for\n * computing the inscriptions on any blockchain network on which the protocol\n * is deployed.\n * @param files A list a files (with their path relative to the root of the\n * directory and their byte content)\n * @param chunkSize Maximum size of the chunks in which the file will be divided\n * @returns A root directory inode from which the whole directory tree can be\n * traversed, as it's going to be inscribed.\n */\nexport async function prepareDirectory(\n  files: IFile[],\n  chunkSize: number = DEFAULT_CHUNK_SIZE\n): Promise<DirectoryInode> {\n  const [graph, leaves] = buildDirectoryGraph(files)\n\n  const parsed: PrepareDirectoryNode[] = []\n  let parsing: PrepareDirectoryNode[] = leaves\n\n  while (parsing.length > 0) {\n    const nextParse: PrepareDirectoryNode[] = []\n    for (const node of parsing) {\n      // if this node has already been parsed, ignore\n      if (parsed.includes(node)) continue\n      if (node.type === \"file\") {\n        node.inode = await prepareFile(\n          {\n            path: node.name,\n            content: node.content,\n          },\n          chunkSize\n        )\n      } else if (node.type === \"directory\") {\n        // compute the inode associated with the directory\n        node.inode = computeDirectoryInode(node)\n      }\n      // marked the node as parsed\n      parsed.push(node)\n      // push the eventual parent to the nodes to parse; eventually when\n      // reaching the head, nothing will have to get parsed\n      if (node.parent) {\n        // we can only push the parent when all its children have been parsed\n        // already (which is checked if .inode property exists)\n        const children = Object.values(node.parent.files)\n        if (!children.find(child => !child.inode)) {\n          nextParse.push(node.parent)\n        }\n      }\n    }\n    // once all the nodes to parse have been parsed, assign the next wave\n    parsing = nextParse\n  }\n\n  // at this point graph.inodes has been populated with the root directory node,\n  // which happens to be linked to the rest of the inodes; it can returned\n  return graph.inode!\n}\n","import { DEFAULT_CHUNK_SIZE } from \"@/config\"\nimport {\n  DirectoryInode,\n  FileInode,\n  IFile,\n  OnchfsPrepareOptions,\n} from \"@/types/files\"\nimport { prepareDirectory } from \"./directory\"\nimport { prepareFile } from \"./file\"\n\nconst defaultPrepareOptions: Required<OnchfsPrepareOptions> = {\n  chunkSize: DEFAULT_CHUNK_SIZE,\n}\n\nexport function prepare(file: IFile, options: OnchfsPrepareOptions): FileInode\nexport function prepare(\n  files: IFile[],\n  options: OnchfsPrepareOptions\n): DirectoryInode\n\n/**\n * Prepares some files for their inscription on the onchfs. If a list of files\n * is provided, will create a directory in which files will be located at its\n * riit.\n * @param files file or list of files to for which insertions to prepare\n * inscriptions\n * @param options Options object to modular behaviour\n * @returns A grapg of nodes/chunks to insert, built as an inverted tree with\n * the upmost node of the tree (if file, file node, otherwise a directory)\n */\nexport function prepare(\n  files: IFile | IFile[],\n  options: OnchfsPrepareOptions\n): unknown {\n  const _options = {\n    ...defaultPrepareOptions,\n    ...options,\n  }\n  if (Array.isArray(files)) {\n    return prepareDirectory(files, _options.chunkSize)\n  } else {\n    return prepareFile(files, _options.chunkSize)\n  }\n}\n","export { inscriptionsBytesLength } from \"./estimate\"\nexport { prepareInscriptions as prepare } from \"./prepare\"\n","import { Inscription } from \"@/types/inscriptions\"\n\n/**\n * Compute the number of bytes an inscription will take on the storage.\n * @param ins Inscription for which storage space should be computed\n * @returns The number of storage bytes the inscription will take\n */\nfunction inscriptionBytesLength(ins: Inscription) {\n  switch (ins.type) {\n    case \"chunk\":\n      // chunk to write + chunk key\n      return ins.content.byteLength + 32\n    case \"directory\":\n      // for every file in directory:\n      //  - 32 bytes for pointer\n      //  - 1 byte per character (stored in 7-bit ASCII)\n      // and 32 bytes for the directory pointer\n      return (\n        Object.keys(ins.files)\n          .map(name => name.length + 32)\n          .reduce((a, b) => a + b, 0) + 32\n      )\n    case \"file\":\n      // 32 bytes per chunk + metadata + 32 bytes for pointer\n      return (\n        32 + // 32 bytes for pointer\n        32 * ins.chunks.length + // 32 bytes per chunk\n        ins.metadata.byteLength\n      )\n  }\n}\n\n/**\n * Computes the maximum number of storage bytes which will be consumed by the\n * inscriptions when they are written on-chain. This is a maximum value, as\n * some chunks/files/directories may already have been written to the storage.\n * Note: this doesn't include eventual gas execution fees, which are blockchain-\n * dependant.\n * @param inscriptions Inscriptions for which storage bytes will be computed\n * @returns Number of bytes the inscriptions may take on the storage\n */\nexport function inscriptionsBytesLength(inscriptions: Inscription[]) {\n  return inscriptions.reduce((acc, ins) => inscriptionBytesLength(ins) + acc, 0)\n}\n","import { INode } from \"@/types/files\"\nimport { Inscription } from \"@/types/inscriptions\"\n\n/**\n * Traverse the inverted tree starting by the root, creating inscriptions as\n * it's being traversed. At the end of the flow the inscriptions will be\n * reversed to ensure they are written to the store in the right order (as the\n * onchfs will reject inodes pointing to inexisting resources; let it be file\n * chunks or directory files).\n * @param root The root of the tree, can be either the root directory or a file\n * @returns A list of inscription objects ready to be turned into operations\n */\nexport function prepareInscriptions(root: INode): Inscription[] {\n  const inscriptions: Inscription[] = []\n  const traverse = (node: INode) => {\n    if (node.type === \"directory\") {\n      inscriptions.push({\n        type: \"directory\",\n        files: Object.fromEntries(\n          Object.keys(node.files).map(name => [name, node.files[name].cid])\n        ),\n        cid: node.cid,\n      })\n      // recursively traverse each inode of the directory\n      for (const name in node.files) {\n        traverse(node.files[name])\n      }\n    } else if (node.type === \"file\") {\n      // create the file inscription first as it will be reversed in the end,\n      // so the chunk inscriptions will appear first\n      inscriptions.push({\n        type: \"file\",\n        chunks: node.chunks.map(chk => chk.hash),\n        metadata: node.metadata,\n        cid: node.cid,\n      })\n      for (const chunk of node.chunks) {\n        inscriptions.push({\n          type: \"chunk\",\n          content: chunk.bytes,\n        })\n      }\n    }\n  }\n  traverse(root)\n  return inscriptions.reverse()\n}\n","import { validateMetadataValue } from \"./validation\"\n\nexport { encodeMetadata as encode } from \"./encode\"\nexport { decodeMetadata as decode } from \"./decode\"\n\nexport const utils = {\n  validateMetadataValue,\n}\n","import { FileMetadataEntries } from \"@/types/metadata\"\nimport hpack from \"hpack.js\"\n\n/**\n * Decodes the HPACKed metadata into a POJO, where keys are the header keys,\n * and value their respective string value.\n * @param raw The raw bytes of the hpack-encoded metadata\n * @returns POJO of metadata header\n */\nexport function decodeMetadata(raw: Uint8Array): FileMetadataEntries {\n  const decomp = hpack.decompressor.create({ table: { size: 256 } })\n  const metadata: FileMetadataEntries = {}\n  decomp.write(raw)\n  decomp.execute()\n  let buff: any\n  while ((buff = decomp.read())) {\n    metadata[buff.name] = buff.value\n  }\n  return metadata\n}\n","export * from \"./errors\"\nexport * from \"./proxy\"\nexport * from \"../uri\"\n","import { ProxyResolutionStatusErrors } from \"@/types/resolver\"\n\n/**\n * Error thrown during the resolution of a relative URI by the proxy.\n */\nexport class OnchfsProxyResolutionError extends Error {\n  constructor(message: string, public status: ProxyResolutionStatusErrors) {\n    super(message)\n    this.name = \"OnchfsProxyResolutionError\"\n  }\n}\n","import { FileMetadataEntries } from \"./metadata\"\nimport { URIAuthority } from \"./uri\"\n\nexport interface InodeFileNativeFS {\n  cid: string\n  metadata: string | Uint8Array\n  chunkPointers: string[]\n}\n\nexport interface InodeDirectoryNativeFS {\n  cid: string\n  files: Record<string, string>\n}\n\nexport type InodeNativeFS = InodeFileNativeFS | InodeDirectoryNativeFS\n\nexport interface Resolver {\n  getInodeAtPath: (\n    cid: string,\n    path: string[],\n    authority?: URIAuthority\n  ) => Promise<InodeNativeFS | null>\n  readFile: (\n    cid: string,\n    chunkPointers: string[],\n    authority?: URIAuthority\n  ) => Promise<string | Uint8Array>\n}\n\nexport enum ProxyResolutionStatusErrors {\n  NOT_ACCEPTABLE = 406,\n  NOT_FOUND = 404,\n  BAD_REQUEST = 400,\n  INTERNAL_SERVER_ERROR = 500,\n}\n\nexport enum ProxyResolutionStatusSuccess {\n  OK = 200,\n}\n\nexport enum ProxyResolutionStatusRedirect {\n  PERMANENT = 308,\n}\n\nexport type ProxyResolutionStatus =\n  | ProxyResolutionStatusSuccess\n  | ProxyResolutionStatusErrors\n  | ProxyResolutionStatusRedirect\n\nexport interface ProxyResolutionError {\n  code: number\n  name: string\n  message?: string\n}\n\nexport interface ProxyExtraHeaders {\n  Location: string\n}\n\nexport type ProxyHttpHeaders = FileMetadataEntries | ProxyExtraHeaders\n\nexport interface ProxyResolutionResponse {\n  status: ProxyResolutionStatus\n  content: Uint8Array\n  headers: ProxyHttpHeaders\n  error?: ProxyResolutionError\n}\n","/**\n * List of the blockchain supported officially. While the protocol can be\n * deployed anywhere, the URI resolution is more easily inferred from the\n * supported deployments.\n */\nexport const blockchainNames = [\"tezos\", \"ethereum\"] as const\nexport type BlockchainNames = (typeof blockchainNames)[number]\n\n/**\n * The URI Authority defines the \"host\" of an asset, in this case a combination\n * of a blockchain (represented by string indentifier & chain ID) & a contract\n * on such blockchain compliant to the onchfs specifications.\n */\nexport interface URIAuthority {\n  blockchainName: string\n  contract: string\n  blockchainId: string\n}\n\n/**\n * Provides a full broken-down representation of the URI in a formatted way.\n * While the authority is an optionnal segment when writing an onchfs URI, it\n * must be inferred during the resolution by providing a context.\n */\nexport interface URIComponents {\n  cid: string\n  authority: URIAuthority\n  path?: string\n  query?: string\n  fragment?: string\n}\n\n/**\n * Some URI Context needs to be provided for most URI resolutions, as often\n * onchfs URI will rely of the context in which they are stored/seen to infer\n * their resolution. For instance, a base onchfs URI stored on ETH mainnet will\n * expect solvers to point to the main onchfs ETH contract.\n */\nexport type URIContext = Pick<URIAuthority, \"blockchainName\"> &\n  Pick<Partial<URIAuthority>, \"blockchainId\" | \"contract\">\n\n/**\n * The different segments of the URI Schema-Specific Component:\n * [ authority \"/\" ] cid [path] [ \"?\" query ] [ \"#\" fragment ]\n */\nexport interface URISchemaSpecificParts {\n  cid: string\n  authority?: string\n  path?: string\n  query?: string\n  fragment?: string\n}\n","/**\n * Proper charsets tightly following the spec\n */\n\nimport { DEFAULT_CONTRACTS } from \"@/config\"\nimport {\n  BlockchainNames,\n  URIAuthority,\n  URIComponents,\n  URIContext,\n  URISchemaSpecificParts,\n  blockchainNames,\n} from \"@/types/uri\"\n\nconst LOW_ALPHA = \"a-z\"\nconst HI_ALPHA = \"A-Z\"\nconst ALPHA = LOW_ALPHA + HI_ALPHA\nconst DIGIT = \"0-9\"\nconst SAFE = \"$\\\\-_.+\"\nconst EXTRA = \"!*'(),~\"\nconst HEX_CHARSET = \"A-Fa-f0-9\"\nconst LOW_RESERVED = \";:@&=\"\nconst RESERVED = LOW_RESERVED + \"\\\\/?#\"\nconst UNRESERVED = ALPHA + DIGIT + SAFE + EXTRA\nconst PCT_ENCODED = `%[${HEX_CHARSET}]{2}`\nconst UCHAR = `(?:(?:[${UNRESERVED}])|(?:${PCT_ENCODED}))`\nconst XCHAR = `(?:(?:[${UNRESERVED}${RESERVED}])|(?:${PCT_ENCODED}))`\n\nconst URI_CHARSET = XCHAR\nconst B58_CHARSET = \"1-9A-HJ-NP-Za-km-z\"\nconst AUTHORITY_CHARSET = `${HEX_CHARSET}${B58_CHARSET}.a-z:`\nconst SEG_CHARSET = `(?:(?:${UCHAR})|[${LOW_RESERVED}])`\nconst QUERY_CHARSET = `(?:${SEG_CHARSET}|\\\\/|\\\\?)`\n\n/**\n * Parses an absolute onchfs URI, following its ABNF specification. If any part\n * of the URI is mal-constructed, or if some context is missing to fully\n * resolve it, this function will throw with an error indicating where the\n * resolution has failed.\n *\n * @dev The resolution happens in 3 distinctive steps, each breaking down the\n * URI into smaller components which can be parsed more easily:\n *\n * * parseSchema\n *   <onchfs>://<schema-specific-part>\n *   Splits the URI into it's 2 biggest sections, extracting the schema from\n *   the schema-specific part.\n * * parseSchemaSpecificPart\n *   [ authority \"/\" ] cid [path] [ \"?\" query ] [ \"#\" fragment ]\n *   Splits the schema-specific part into its various logical segments. This\n *   step validate the general structure of the URI as well.\n * * parseAuthority\n *   [ contract-address \".\" ] blockchain-name [ \":\" chainid ]\n *   The authority segment is parsed, and using the context provided by the\n *   consumer it tries to resolve the authority.\n *\n * @param uri An absolute onchfs URI to be parsed\n * @param context The context in which the URI has been observed; such context\n * may be required for resolving the URI as it might be written in its short\n * form to save storage space, relying on inference from context; which is\n * valid part of the spec designed to optimise storage costs.\n * @returns A fully formed URI Components object, describing the various URI\n * components.\n */\nexport function parseURI(uri: string, context?: URIContext): URIComponents {\n  try {\n    // pass the first layer: validates the high level format of the URI:\n    // onchfs://<schema-specific-part>\n    // and returns the schema-specific part if the URI has a general valid\n    // format.\n    const schemaSpecificPart = parseSchema(uri)\n    // parse the schema-specific part\n    // [ authority \"/\" ] cid [path] [ \"?\" query ] [ \"#\" fragment ]\n    const schemaSegments = parseSchemaSpecificPart(schemaSpecificPart)\n    // parse the authority\n    const authority = parseAuthority(schemaSegments.authority, context)\n\n    return {\n      ...schemaSegments,\n      cid: schemaSegments.cid.toLowerCase(),\n      authority: authority,\n    }\n  } catch (err: any) {\n    // catch to prefix low-level error message with generic message\n    throw new Error(\n      `Error when parsing the URI \"${uri}\" as a onchfs URI: ${err.message}`\n    )\n  }\n}\n\n/**\n * 1st order URI parsing; checks if the overall URI is valid by looking at the\n * protocol, and the schema-specific part. If any character in the URI invalid\n * (not part of the allowed URI characters), will throw.\n * Will also thrown if the general pattern doesn't not comply with onchfs URIs.\n * @param uri The URI to parse\n * @returns The URI schema-specific part\n */\nexport function parseSchema(uri: string): string {\n  const regex = new RegExp(`^(onchfs):\\/\\/(${URI_CHARSET}{64,})$`)\n  const results = regex.exec(uri)\n\n  // result is null, the regex missed\n  if (!results) {\n    throw new Error(\n      `general onchfs URI format is invalid / Pattern didn't match: ${regex.toString()}`\n    )\n  }\n\n  return results[2]\n}\n\n/**\n * Parses the schema-specific component (onchfs://<schema-specific-component>)\n * into a list of sub-segments based on the onchfs URI specification.\n * [ authority \"/\" ] cid [path] [ \"?\" query ] [ \"#\" fragment ]\n * @param uriPart THe URI Schema-Specific Component\n * @returns An object with the different segments isolated\n */\nexport function parseSchemaSpecificPart(\n  uriPart: string\n): URISchemaSpecificParts {\n  const authorityReg = `([${AUTHORITY_CHARSET}]*)\\\\/`\n  const cidReg = `[${HEX_CHARSET}]{64}`\n  const pathReg = `${SEG_CHARSET}*(?:\\\\/${SEG_CHARSET}*)*`\n  const queryReg = `\\\\?(${QUERY_CHARSET}*)`\n  const fragReg = `#(${QUERY_CHARSET}*)`\n\n  // isolates each segment of the URI based on their pattern, including\n  // cardinality of every segment\n  const regex = new RegExp(\n    `^(?:${authorityReg})?(${cidReg})(?:\\\\/(${pathReg}))?(?:${queryReg})?(?:${fragReg})?$`\n  )\n\n  const res = regex.exec(uriPart)\n\n  if (!res) {\n    throw new Error(\n      `the URI schema specific component seems to be invalid. \"${uriPart}\" should respect the following pattern: ${regex.toString()}`\n    )\n  }\n\n  const [_, authority, cid, path, query, fragment] = res\n\n  return {\n    authority,\n    cid,\n    path,\n    query,\n    fragment,\n  }\n}\n\nconst blockchainAuthorityParsers: Record<BlockchainNames, () => RegExp> = {\n  tezos: () =>\n    new RegExp(\n      `^(?:(KT(?:1|2|3|4)[${B58_CHARSET}]{33})\\\\.)?(tezos|tez|xtz)(?::(ghostnet|mainnet))?$`\n    ),\n  ethereum: () =>\n    new RegExp(`^(?:([${HEX_CHARSET}]{40})\\\\.)?(ethereum|eth)(?::([0-9]+))?$`),\n}\n\ntype BlockchainNameVariants = {\n  [K in BlockchainNames]: [K, ...string[]]\n}\nconst blockchainNameVariants: BlockchainNameVariants = {\n  tezos: [\"tezos\", \"tez\", \"xtz\"],\n  ethereum: [\"ethereum\", \"eth\"],\n}\n\ntype BlockchainDefaultNetwork = {\n  [K in BlockchainNames]: string\n}\nconst blockchainDefaultNetwork: BlockchainDefaultNetwork = {\n  tezos: \"mainnet\",\n  ethereum: \"1\",\n}\n\n/**\n * Given the string segment of the authority (or lack thereof) and a context in\n * which the URI exists, resolves the authority object (blockchain, chainid,\n * smart contract address) in which the object is supposed to be stored.\n *\n * The resolution is initialized with the provided context, after which the\n * authority segment of the URI (onchfs://<authority>/<cid>/<path>...) is parsed\n * and eventually overrides the given context (as some resources living in a\n * given context are allowed to reference assets existing in other contexts).\n *\n * If some authority components are still missing after the parsing, the\n * blockchain name is used to infer (chainid, contract address). In case\n * only contract is missing, it is inferred from (blockchain name, chainid).\n *\n * If any component is missing at the end of this process (ie: cannot be found\n * in the context, URI, and cannot be inferred), this functions throws.\n *\n * @param authority The string segment of the authority in the URI. If the\n * authority is missing from the CID, a context must be provided to resolve\n * the authority component.\n * @param context The context in which the authority is loaded. If such context\n * is not provided, the authority segment must have a blockchain name so that\n * the authority can be resolved using defaults.\n * @returns An object containing all the segments of the authority. If segment\n * doesn't exist, the context provided will be used to infer all the authority\n * components.\n */\nexport function parseAuthority(\n  authority?: string,\n  context?: URIContext\n): URIAuthority {\n  // initialise the authority object to the given context\n  let tmp: Partial<URIAuthority> = { ...context }\n\n  if (authority) {\n    // loop through every blockchain and use its authority-regex to identify\n    // the different parts, potentially\n    let regex: RegExp, res: RegExpExecArray | null\n    for (const name of blockchainNames) {\n      // generate the blockchain-related regex and parse the authority\n      regex = blockchainAuthorityParsers[name]()\n      res = regex.exec(authority)\n      // no result; move to next blockchain\n      if (!res) continue\n      // results are in slots [1;3] - assign to temp object being parsed\n      const [contract, blockchainName, blockchainId] = res.splice(1, 3)\n      contract && (tmp.contract = contract)\n      blockchainName && (tmp.blockchainName = blockchainName)\n      blockchainId && (tmp.blockchainId = blockchainId)\n      break\n    }\n  }\n\n  // at this stage, if there isn't a blockchain name, then we can throw as the\n  // network is missing (either from the URI or the context)\n  if (!tmp.blockchainName) {\n    throw new Error(\n      \"the blockchain could not be inferred when parsing the URI, if the URI doesn't have an authority segment (onchfs://<authority>/<cid>/<path>...), a context should be provided based on where the URI was observed. The blockchain needs to be resolved either through the URI or using the context.\"\n    )\n  }\n  // normalize blockchain name into its cleanest and most comprehensible form\n  for (const [name, values] of Object.entries(blockchainNameVariants)) {\n    if (values.includes(tmp.blockchainName)) {\n      tmp.blockchainName = name\n      break\n    }\n  }\n\n  // if blockchain ID is missing, then assign the default blockchain ID\n  // associated with the asset, which is mainnet\n  if (!tmp.blockchainId) {\n    tmp.blockchainId = blockchainDefaultNetwork[tmp.blockchainName]\n  }\n  // if no blockchain ID at this stage, the blockchain name cannot be mapped\n  // to a given blockchain ID\n  if (!tmp.blockchainId) {\n    throw new Error(\n      `The blockchain identifier could not be inferred from the blockchain name when parsing the authority segment of the URI. This can happen when a blockchain not supported by the onchfs package was provided in the context of the URI resolution, yet a blockchain ID wasn't provided in the context nor could it be found in the URI.`\n    )\n  }\n\n  // if contract is missing, default one for blockchain (name, id) is picked\n  if (!tmp.contract) {\n    tmp.contract =\n      DEFAULT_CONTRACTS[`${tmp.blockchainName}:${tmp.blockchainId}`]\n  }\n  // if no contract at this stage, it's mostly because some unofficial context\n  // has been forced, yet no contract was given in the context\n  if (!tmp.contract) {\n    throw new Error(\n      `A File Object contract could not be associated with the onchfs URI. This can happen when an unsupported blockchain was provided as a context to the URI resolver, yet no contract was provided in the context, nor could it be parsed from the URI. The URI must resolve to a Smart Contract.`\n    )\n  }\n\n  return tmp as URIAuthority\n}\n","import { hexStringToBytes } from \"@/utils\"\nimport { OnchfsProxyResolutionError } from \"./errors\"\nimport { decodeMetadata } from \"@/metadata/decode\"\nimport {\n  InodeDirectoryNativeFS,\n  InodeFileNativeFS,\n  InodeNativeFS,\n  ProxyHttpHeaders,\n  ProxyResolutionError,\n  ProxyResolutionResponse,\n  ProxyResolutionStatus,\n  ProxyResolutionStatusErrors,\n  ProxyResolutionStatusRedirect,\n  ProxyResolutionStatusSuccess,\n  Resolver,\n} from \"@/types/resolver\"\nimport { URIAuthority, URISchemaSpecificParts } from \"@/types/uri\"\nimport {\n  parseAuthority,\n  parseSchema,\n  parseSchemaSpecificPart,\n} from \"@/uri/parse\"\n\nconst ResolutionErrors: Record<ProxyResolutionStatusErrors, string> = {\n  [ProxyResolutionStatusErrors.BAD_REQUEST]: \"Bad Request\",\n  [ProxyResolutionStatusErrors.NOT_ACCEPTABLE]: \"Resource Cannot be Served\",\n  [ProxyResolutionStatusErrors.NOT_FOUND]: \"Not Found\",\n  [ProxyResolutionStatusErrors.INTERNAL_SERVER_ERROR]: \"Internal Server Error\",\n}\n\n/**\n * Creates a generic-purpose URI resolver, leaving the implementation of\n * the file system resource resolution to the consumer of this API. The\n * resolver is a function of URI, which executes a series of file system\n * operations based on the URI content and the responses from the file\n * system.\n *\n * @param resolver An object which implements low-level retrieval methods to\n * fetch the necessary content from the file system in order to resolve the\n * URI.\n * @returns A function which takes an URI as an input and executes a serie of\n * operations to retrieve the file targetted by the URI. The resolution\n * of the file pointers against the file system is left to the consuming\n * application, which can implement different strategies based on the\n * use-cases.\n */\nexport function createProxyResolver(resolver: Resolver) {\n  /**\n   * A function which takes an URI as an input and executes a serie of\n   * operations to retrieve the file targetted by the URI. The resolution\n   * of the file pointers against the file system is left to the consuming\n   * application, which can implement different strategies based on the\n   * use-cases.\n   * @param uri An URI (schema-prefixed or not) pointing to the asset.\n   */\n  return async (uri: string): Promise<ProxyResolutionResponse> => {\n    try {\n      if (uri.startsWith(\"/\")) {\n        uri = uri.slice(1)\n      }\n      // extract the main components of interest: path & cid\n      let components: URISchemaSpecificParts\n      try {\n        // if for some reason an absolute URI was provided at the proxy-\n        // resolution level, extract it from the URI which will get parsed\n        if (uri.startsWith(\"onchfs://\")) {\n          uri = parseSchema(uri)\n        }\n        // only the schema-specific part is parsed\n        components = parseSchemaSpecificPart(uri)\n      } catch (err) {\n        throw new OnchfsProxyResolutionError(\n          `The onchfs URI is invalid: ${err.message}`,\n          ProxyResolutionStatusErrors.BAD_REQUEST\n        )\n      }\n\n      let { cid, path, authority } = components\n      // path segments are separated in array, empty segments are filtered out\n      let paths = path?.split(\"/\") || []\n      paths = paths.filter(pt => pt.length > 0)\n\n      // if there's an authority, parse it\n      let parsedAuthority: URIAuthority | undefined\n      if (authority) {\n        parsedAuthority = parseAuthority(authority)\n      }\n\n      // resolve the inode at the given target\n      let inode: InodeNativeFS\n      let mainInode: InodeNativeFS\n      try {\n        inode = mainInode = await resolver.getInodeAtPath(\n          cid,\n          paths,\n          parsedAuthority\n        )\n        if (!inode) {\n          throw new OnchfsProxyResolutionError(\n            `Could not find any inode at (${cid}, ${path})`,\n            ProxyResolutionStatusErrors.NOT_FOUND\n          )\n        }\n      } catch (err) {\n        // if the error is already an instance of ProxyResolutionError, simply\n        // forward it, otherwise craft a 500 error and throw it\n        if (err instanceof OnchfsProxyResolutionError) {\n          throw err\n        } else {\n          throw new OnchfsProxyResolutionError(\n            err.message,\n            ProxyResolutionStatusErrors.INTERNAL_SERVER_ERROR\n          )\n        }\n      }\n\n      // if the inode is a directory, try to get the index.html file\n      if ((inode as InodeDirectoryNativeFS).files) {\n        if ((inode as InodeDirectoryNativeFS).files[\"index.html\"]) {\n          try {\n            inode = await resolver.getInodeAtPath(\n              inode.cid,\n              [\"index.html\"],\n              parsedAuthority\n            )\n          } catch (err) {\n            // throw an internal server error\n            throw new OnchfsProxyResolutionError(\n              `An error occurred when resolving the index.html file inside the target directory at /${\n                inode.cid\n              }${err.message ? `: ${err.message}` : \"\"}`,\n              ProxyResolutionStatusErrors.INTERNAL_SERVER_ERROR\n            )\n          }\n        }\n        // if it's a directory but no index.html file at its root, throw\n        else {\n          throw new OnchfsProxyResolutionError(\n            `the inode at (${cid}, ${path}) is a directory which doesn't contain any index.html file, as such it cannot be served.`,\n            ProxyResolutionStatusErrors.NOT_ACCEPTABLE\n          )\n        }\n      }\n\n      // if the inode at this stage is a directory, throw\n      if ((inode as InodeDirectoryNativeFS).files || !inode) {\n        throw new OnchfsProxyResolutionError(\n          `could not find a file inode at (${cid}, ${path})`,\n          ProxyResolutionStatusErrors.NOT_FOUND\n        )\n      }\n\n      // fetch the file content (eventually normalize data to Uint8Array)\n      let content: Uint8Array\n      try {\n        const contentInput = await resolver.readFile(\n          inode.cid,\n          (inode as InodeFileNativeFS).chunkPointers,\n          parsedAuthority\n        )\n        content =\n          typeof contentInput === \"string\"\n            ? hexStringToBytes(contentInput)\n            : contentInput\n      } catch (err) {\n        throw new OnchfsProxyResolutionError(\n          `An error occurred when reading the content of the file of cid ${\n            inode.cid\n          }${err.message ? `: ${err.message}` : \"\"}`,\n          ProxyResolutionStatusErrors.INTERNAL_SERVER_ERROR\n        )\n      }\n\n      // parse the metadata from the file into an object header ready for an\n      // http response\n      let headers: ProxyHttpHeaders\n      const rawMetadataInput = (inode as InodeFileNativeFS).metadata\n      try {\n        const rawMetadata =\n          typeof rawMetadataInput === \"string\"\n            ? hexStringToBytes(rawMetadataInput)\n            : rawMetadataInput\n        headers = decodeMetadata(rawMetadata)\n      } catch (err) {\n        throw new OnchfsProxyResolutionError(\n          `An error occurred when parsing the metadata of the file of cid ${\n            inode.cid\n          }, raw metadata bytes (${rawMetadataInput})${\n            err.message ? `: ${err.message}` : \"\"\n          }`,\n          ProxyResolutionStatusErrors.INTERNAL_SERVER_ERROR\n        )\n      }\n\n      let status: ProxyResolutionStatus = ProxyResolutionStatusSuccess.OK\n\n      // if the main inode is a directory, & the given path doesn't end with a\n      // trailing slash, we enforce a trailing slash at the end of the path to\n      // ensure that browsers relative URL resolution will resolve files\n      // relative to the one served starting at the end of the path.\n      //\n      // ex: /aeaeaeaeaeae   -> points to /aeaeaeaeaeae/index.html\n      // ->  /aeaeaeaeaeae/  -> redirect to\n      const wholeReqPath =\n        components.cid + (components.path ? `/${components.path}` : \"\")\n      if (\n        (mainInode as InodeDirectoryNativeFS).files &&\n        !wholeReqPath.endsWith(\"/\") &&\n        // in case a CID is followed by a \"/\" and an empty path, the \"/\" will\n        // disappear during parsing due to the URI specification, however an\n        // empty string path will appear, which is the only case where it\n        // appears; as such we can test it to cover this edge-case.\n        !(components.path === \"\")\n      ) {\n        const redirect =\n          \"/\" +\n          components.cid +\n          (components.path ? `/${components.path}` : \"\") +\n          \"/\" +\n          (components.query ? `?${components.query}` : \"\")\n        // add the Location header for redirect\n        headers = {\n          // preserve the existing headers; we will still be serving the content\n          ...headers,\n          Location: redirect,\n        }\n        status = ProxyResolutionStatusRedirect.PERMANENT\n      }\n\n      // everything is OK, the response can be served\n      return {\n        content,\n        headers,\n        status,\n      }\n    } catch (err) {\n      // an error was found; it's very likely that it's a ProxyResolutionError\n      // instance yet we procide a catch-all to ensure a 500 is returned if\n      // the errors happens to be something else, unexpected\n      let status: ProxyResolutionStatusErrors, error: ProxyResolutionError\n      if (err instanceof OnchfsProxyResolutionError) {\n        status = err.status\n        error = {\n          code: err.status,\n          name: ResolutionErrors[err.status],\n          message: err.message,\n        }\n      } else {\n        status = ProxyResolutionStatusErrors.INTERNAL_SERVER_ERROR\n        error = {\n          code: ProxyResolutionStatusErrors.INTERNAL_SERVER_ERROR,\n          name: ResolutionErrors[\n            ProxyResolutionStatusErrors.INTERNAL_SERVER_ERROR\n          ],\n        }\n      }\n      // forward the response\n      return {\n        status,\n        // we produce a basic html error page for proxy implementations which\n        // just want to forward the response\n        content: new TextEncoder().encode(\n          `<h1>${status} ${error.name}</h1>${\n            error.message ? `<p>${err.message}</p>` : \"\"\n          }`\n        ),\n        // same with headers\n        headers: {\n          \"content-type\": \"text/html; charset=utf-8\",\n        },\n        error,\n      }\n    }\n  }\n}\n","import { parseAuthority, parseSchema, parseSchemaSpecificPart } from \"./parse\"\n\nexport { parseURI as parse } from \"./parse\"\n\nexport const utils = {\n  parseAuthority,\n  parseSchema,\n  parseSchemaSpecificPart,\n}\n","import * as files from \"@/files\"\nimport * as inscriptions from \"@/inscriptions\"\nimport * as metadata from \"@/metadata\"\nimport * as resolver from \"@/resolver\"\nimport * as uri from \"@/uri\"\nimport * as utils from \"@/utils\"\n\nconst Onchfs = {\n  files,\n  inscriptions,\n  metadata,\n  resolver,\n  uri,\n  utils,\n}\n\nexport default Onchfs\n\n// export all types for dev convenience\nexport * from \"@/types\"\n\n// Used to expose the library to the browser build version\nif (typeof window !== \"undefined\") {\n  ;(window as any).Onchfs = Onchfs\n}\n"],"mappings":";;;;;;;AAAA;AAAA;AAAA;AAAA;AAAA;;;ACEO,IAAM,wBAAwB;AAAA,EACnC,MAAM,IAAI,WAAW,CAAC,CAAC,CAAC;AAAA,EACxB,WAAW,IAAI,WAAW,CAAC,CAAC,CAAC;AAC/B;AAKO,IAAM,qBAAqB;AAI3B,IAAM,oBAA4C;AAAA,EACvD,iBAAiB;AAAA,EACjB,kBAAkB;AAAA,EAClB,cAAc;AAAA,EACd,cAAc;AAChB;;;ACnBA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;;;ACAA,SAAS,iBAAiB;AAMnB,SAAS,OAAO,OAAwC;AAC7D,SAAO,IAAI,WAAW,UAAU,OAAO,KAAK,CAAC;AAC/C;;;ACDO,SAAS,iBAAiB,KAAyB;AACxD,QAAM,MAAM,IAAI,OAAO,uBAAuB;AAC9C,MAAI,CAAC,IAAI,KAAK,GAAG,GAAG;AAClB,UAAM,IAAI;AAAA,MACR;AAAA,YAAiF,IAAI,SAAS;AAAA,MAAU,IAAI;AAAA,QAC1G;AAAA,QACA;AAAA,MACF,IAAI,IAAI,SAAS,KAAK,QAAQ;AAAA,IAChC;AAAA,EACF;AACA,QAAM,QAAQ,IAAI,WAAW,IAAI,SAAS,CAAC;AAC3C,WAAS,IAAI,GAAG,IAAI,IAAI,SAAS,GAAG,KAAK;AACvC,UAAM,CAAC,IAAI,SAAS,IAAI,MAAM,IAAI,GAAG,IAAI,IAAI,CAAC,GAAG,EAAE;AAAA,EACrD;AACA,SAAO;AACT;;;ACbO,SAAS,gBACd,QACA,SAAS,GACT,QACA;AACA,WAAS,OAAO,WAAW,cAAc,OAAO,aAAa,SAAS;AACtE,QAAM,MAAM,IAAI,WAAW,MAAM;AACjC,WAAS,IAAI,GAAG,IAAI,QAAQ,KAAK;AAC/B,QAAI,CAAC,IAAI,OAAO,IAAI,MAAM;AAAA,EAC5B;AACA,SAAO;AACT;AAQO,SAAS,qBAAqB,QAAkC;AACrE,QAAM,IAAI,OAAO,OAAO,CAAC,KAAK,QAAQ,IAAI,SAAS,KAAK,CAAC;AACzD,QAAM,MAAM,IAAI,WAAW,CAAC;AAC5B,MAAI,SAAS;AACb,aAAW,OAAO,QAAQ;AACxB,QAAI,IAAI,KAAK,MAAM;AACnB,cAAU,IAAI;AAAA,EAChB;AACA,SAAO;AACT;AASO,SAAS,mBAAmB,GAAe,GAAuB;AAEvE,WAAS,IAAI,GAAG,IAAI,EAAE,QAAQ,KAAK;AACjC,QAAI,EAAE,CAAC,IAAI,EAAE,CAAC;AAAG,aAAO;AACxB,QAAI,EAAE,CAAC,IAAI,EAAE,CAAC;AAAG,aAAO;AAAA,EAC1B;AACA,SAAO;AACT;AAUO,SAAS,mBAAmB,GAAe,GAAwB;AACxE,MAAI,EAAE,WAAW,EAAE;AAAQ,WAAO;AAClC,WAAS,IAAI,GAAG,IAAI,EAAE,QAAQ,KAAK;AACjC,QAAI,EAAE,CAAC,MAAM,EAAE,CAAC;AAAG,aAAO;AAAA,EAC5B;AACA,SAAO;AACT;;;AClDO,SAAS,WACd,SACA,YAAoB,oBACP;AACb,MAAI,aAAa,GAAG;AAClB,UAAM,IAAI,MAAM,8CAA8C;AAAA,EAChE;AACA,QAAM,IAAI,QAAQ;AAClB,QAAM,KAAK,KAAK,KAAK,IAAI,SAAS;AAClC,QAAM,SAAsB,CAAC;AAC7B,MAAI;AACJ,WAAS,IAAI,GAAG,IAAI,IAAI,KAAK;AAC3B,YAAQ;AAAA,MACN;AAAA,MACA,IAAI;AAAA,MACJ,KAAK,IAAI,WAAW,IAAI,IAAI,SAAS;AAAA,IACvC;AACA,WAAO,KAAK;AAAA,MACV,OAAO;AAAA,MACP,MAAM,OAAO,KAAK;AAAA,IACpB,CAAC;AAAA,EACH;AACA,SAAO;AACT;;;AC1CA,SAAS,YAAY;AAErB,SAAS,UAAU,kBAAkB;;;ACFrC,OAAOA,YAAW;;;ACEX,IAAM,2BAA2B;AAAA,EACtC;AAAA;AACF;AAQO,SAAS,sBAAsB,OAAqB;AACzD,WAAS,IAAI,GAAG,IAAI,MAAM,QAAQ,KAAK;AACrC,QAAI,yBAAyB,SAAS,MAAM,WAAW,CAAC,CAAC,GAAG;AAC1D,YAAM,IAAI;AAAA,QACR,qCAAqC,MAAM;AAAA,UACzC;AAAA,QACF,kBAAkB;AAAA,MACpB;AAAA,IACF;AAAA,EACF;AACF;;;ACtBA,OAAO,WAAW;AAMX,SAAS,2BAA2B,MAA6B;AACtE,QAAM,OAAO,MAAM,cAAc,EAAE,MAAM;AAAA,IACvC,SAAO,IAAI,SAAS,KAAK,YAAY;AAAA,EACvC;AACA,MAAI,CAAC;AAAM,WAAO;AAClB,SAAO,MAAM,cAAc,EAAE,MAAM,QAAQ,IAAI;AACjD;;;AFCO,SAAS,eAAe,UAA2C;AACxE,QAAM,OAAOC,OAAM,WAAW,OAAO,EAAE,OAAO,EAAE,MAAM,IAAI,EAAE,CAAC;AAC7D,MAAI,UAAiB,CAAC;AACtB,MAAI,MAAc;AAClB,aAAW,SAAS,UAAU;AAC5B,WAAO,MAAM,YAAY;AACzB,YAAQ,SAAS,KAAK;AACtB,QAAI;AACF,4BAAsB,KAAK;AAAA,IAC7B,SAAS,KAAP;AACA,YAAM,IAAI;AAAA,QACR,6CAA6C,WAAW,IAAI;AAAA,MAC9D;AAAA,IACF;AACA,YAAQ,KAAK;AAAA,MACX;AAAA,MACA;AAAA,IACF,CAAC;AAAA,EACH;AAGA,YAAU,QAAQ,KAAK,CAAC,GAAG,MAAM;AAC/B,UAAM,KAAK,2BAA2B,EAAE,IAAI;AAC5C,UAAM,KAAK,2BAA2B,EAAE,IAAI;AAC5C,QAAI,OAAO,QAAQ,OAAO;AAAM,aAAO;AACvC,QAAI,OAAO,QAAQ,OAAO;AAAM,aAAO;AACvC,QAAI,OAAO,QAAQ,OAAO;AAAM,aAAO;AACvC,WAAO,KAAK;AAAA,EACd,CAAC;AACD,OAAK,MAAM,OAAO;AAElB,SAAO,IAAI,WAAW,KAAK,KAAK,CAAC;AACnC;;;ADpBA,eAAsB,YACpB,MACA,YAAoB,oBACA;AACpB,QAAM,EAAE,MAAM,MAAM,QAAQ,IAAI;AAChC,MAAI,WAAgC,CAAC;AACrC,MAAI,iBAAiB;AAErB,MAAI,OAAO,WAAW,IAAI;AAE1B,MAAI,CAAC,MAAM;AAAA,EAUX,OAAO;AACL,aAAS,cAAc,IAAI;AAAA,EAC7B;AAGA,QAAM,aAAa,KAAK,OAAO;AAC/B,MAAI,WAAW,aAAa,eAAe,YAAY;AACrD,qBAAiB;AACjB,aAAS,kBAAkB,IAAI;AAAA,EACjC;AAGA,QAAM,SAAS,WAAW,gBAAgB,SAAS;AAEnD,QAAM,kBAAkB,eAAe,QAAQ;AAG/C,QAAM,cAAc,OAAO,cAAc;AACzC,QAAM,eAAe,OAAO,eAAe;AAC3C,QAAM,MAAM;AAAA,IACV,kBAAkB,sBAAsB,MAAM,aAAa,YAAY;AAAA,EACzE;AAEA,SAAO;AAAA,IACL,MAAM;AAAA,IACN;AAAA,IACA;AAAA,IACA,UAAU;AAAA,EACZ;AACF;;;AItDO,SAAS,eAAe,MAAsB;AACnD,SAAO,mBAAmB,IAAI;AAChC;AAQO,SAAS,sBACd,KACgB;AAChB,QAAM,MAAoB,CAAC;AAC3B,QAAM,YAAY,OAAO,KAAK,IAAI,KAAK,EAAE,KAAK;AAC9C,QAAM,WAAkC,CAAC;AACzC,aAAW,YAAY,WAAW;AAChC,UAAM,QAAQ,IAAI,MAAM,QAAQ,EAAE;AAClC,aAAS,QAAQ,IAAI;AAErB,QAAI,QAAQ,OAAO,QAAQ,CAAC;AAE5B,QAAI,QAAQ,MAAM,GAAG;AAAA,EACvB;AAEA,MAAI,QAAQ,sBAAsB,SAAS;AAC3C,SAAO;AAAA,IACL,MAAM;AAAA,IACN,KAAK,OAAO,kBAAkB,GAAG,GAAG,CAAC;AAAA,IACrC,OAAO;AAAA,EACT;AACF;AAWO,SAAS,oBACd,OAC+C;AAC/C,MAAI,QAA6B;AAAA,IAC/B,MAAM;AAAA,IACN,OAAO,CAAC;AAAA,IACR,QAAQ;AAAA,EACV;AACA,QAAM,SAAiC,CAAC;AAExC,aAAW,QAAQ,OAAO;AACxB,QAAI,SAAS,OACX,OAAe;AACjB,UAAM,gBAAgB,KAAK,KAAK,WAAW,IAAI,IAC3C,KAAK,KAAK,MAAM,CAAC,IACjB,KAAK;AAIT,UAAM,QAAQ,cAAc,MAAM,GAAG,EAAE,IAAI,CAAAC,UAAQ,eAAeA,KAAI,CAAC;AACvE,aAAS,IAAI,GAAG,IAAI,MAAM,QAAQ,KAAK;AACrC,aAAO,MAAM,CAAC;AAEd,UAAI,KAAK,WAAW,GAAG;AACrB,cAAM,IAAI;AAAA,UACR,YAAY,KAAK;AAAA,QACnB;AAAA,MACF;AAEA,UAAI,MAAM,MAAM,SAAS,GAAG;AAG1B,YAAI,OAAO,MAAM,eAAe,IAAI,GAAG;AACrC,gBAAM,IAAI;AAAA,YACR,oBAAoB,KAAK;AAAA,UAC3B;AAAA,QACF;AACA,cAAM,SAA+B;AAAA,UACnC,MAAM;AAAA,UACN,SAAS,KAAK;AAAA,UACd,MAAM;AAAA,UACN,QAAQ;AAAA,QACV;AACA,eAAO,MAAM,IAAI,IAAI;AACrB,eAAO,KAAK,MAAM;AAAA,MACpB,OAEK;AACH,YAAI,OAAO,MAAM,eAAe,IAAI,GAAG;AACrC,mBAAS,OAAO,MAAM,IAAI;AAAA,QAC5B,OAAO;AACL,gBAAM,OAA4B;AAAA,YAChC,MAAM;AAAA,YACN,OAAO,CAAC;AAAA,YACR,QAAQ;AAAA,UACV;AACA,iBAAO,MAAM,IAAI,IAAI;AACrB,mBAAS;AAAA,QACX;AAAA,MACF;AAAA,IACF;AAAA,EACF;AAEA,SAAO,CAAC,OAAO,MAAM;AACvB;AAcA,eAAsB,iBACpB,OACA,YAAoB,oBACK;AACzB,QAAM,CAAC,OAAO,MAAM,IAAI,oBAAoB,KAAK;AAEjD,QAAM,SAAiC,CAAC;AACxC,MAAI,UAAkC;AAEtC,SAAO,QAAQ,SAAS,GAAG;AACzB,UAAM,YAAoC,CAAC;AAC3C,eAAW,QAAQ,SAAS;AAE1B,UAAI,OAAO,SAAS,IAAI;AAAG;AAC3B,UAAI,KAAK,SAAS,QAAQ;AACxB,aAAK,QAAQ,MAAM;AAAA,UACjB;AAAA,YACE,MAAM,KAAK;AAAA,YACX,SAAS,KAAK;AAAA,UAChB;AAAA,UACA;AAAA,QACF;AAAA,MACF,WAAW,KAAK,SAAS,aAAa;AAEpC,aAAK,QAAQ,sBAAsB,IAAI;AAAA,MACzC;AAEA,aAAO,KAAK,IAAI;AAGhB,UAAI,KAAK,QAAQ;AAGf,cAAM,WAAW,OAAO,OAAO,KAAK,OAAO,KAAK;AAChD,YAAI,CAAC,SAAS,KAAK,WAAS,CAAC,MAAM,KAAK,GAAG;AACzC,oBAAU,KAAK,KAAK,MAAM;AAAA,QAC5B;AAAA,MACF;AAAA,IACF;AAEA,cAAU;AAAA,EACZ;AAIA,SAAO,MAAM;AACf;;;AChLA,IAAM,wBAAwD;AAAA,EAC5D,WAAW;AACb;AAkBO,SAAS,QACd,OACA,SACS;AACT,QAAM,WAAW;AAAA,IACf,GAAG;AAAA,IACH,GAAG;AAAA,EACL;AACA,MAAI,MAAM,QAAQ,KAAK,GAAG;AACxB,WAAO,iBAAiB,OAAO,SAAS,SAAS;AAAA,EACnD,OAAO;AACL,WAAO,YAAY,OAAO,SAAS,SAAS;AAAA,EAC9C;AACF;;;AZnCO,IAAM,QAAQ;AAAA,EACnB;AAAA,EACA;AAAA,EACA;AAAA,EACA;AACF;;;AabA;AAAA;AAAA;AAAA;AAAA;;;ACOA,SAAS,uBAAuB,KAAkB;AAChD,UAAQ,IAAI,MAAM;AAAA,IAChB,KAAK;AAEH,aAAO,IAAI,QAAQ,aAAa;AAAA,IAClC,KAAK;AAKH,aACE,OAAO,KAAK,IAAI,KAAK,EAClB,IAAI,UAAQ,KAAK,SAAS,EAAE,EAC5B,OAAO,CAAC,GAAG,MAAM,IAAI,GAAG,CAAC,IAAI;AAAA,IAEpC,KAAK;AAEH,aACE;AAAA,MACA,KAAK,IAAI,OAAO;AAAA,MAChB,IAAI,SAAS;AAAA,EAEnB;AACF;AAWO,SAAS,wBAAwB,cAA6B;AACnE,SAAO,aAAa,OAAO,CAAC,KAAK,QAAQ,uBAAuB,GAAG,IAAI,KAAK,CAAC;AAC/E;;;AC/BO,SAAS,oBAAoB,MAA4B;AAC9D,QAAM,eAA8B,CAAC;AACrC,QAAM,WAAW,CAAC,SAAgB;AAChC,QAAI,KAAK,SAAS,aAAa;AAC7B,mBAAa,KAAK;AAAA,QAChB,MAAM;AAAA,QACN,OAAO,OAAO;AAAA,UACZ,OAAO,KAAK,KAAK,KAAK,EAAE,IAAI,UAAQ,CAAC,MAAM,KAAK,MAAM,IAAI,EAAE,GAAG,CAAC;AAAA,QAClE;AAAA,QACA,KAAK,KAAK;AAAA,MACZ,CAAC;AAED,iBAAW,QAAQ,KAAK,OAAO;AAC7B,iBAAS,KAAK,MAAM,IAAI,CAAC;AAAA,MAC3B;AAAA,IACF,WAAW,KAAK,SAAS,QAAQ;AAG/B,mBAAa,KAAK;AAAA,QAChB,MAAM;AAAA,QACN,QAAQ,KAAK,OAAO,IAAI,SAAO,IAAI,IAAI;AAAA,QACvC,UAAU,KAAK;AAAA,QACf,KAAK,KAAK;AAAA,MACZ,CAAC;AACD,iBAAW,SAAS,KAAK,QAAQ;AAC/B,qBAAa,KAAK;AAAA,UAChB,MAAM;AAAA,UACN,SAAS,MAAM;AAAA,QACjB,CAAC;AAAA,MACH;AAAA,IACF;AAAA,EACF;AACA,WAAS,IAAI;AACb,SAAO,aAAa,QAAQ;AAC9B;;;AC9CA;AAAA;AAAA;AAAA;AAAA,eAAAC;AAAA;;;ACCA,OAAOC,YAAW;AAQX,SAAS,eAAe,KAAsC;AACnE,QAAM,SAASA,OAAM,aAAa,OAAO,EAAE,OAAO,EAAE,MAAM,IAAI,EAAE,CAAC;AACjE,QAAM,WAAgC,CAAC;AACvC,SAAO,MAAM,GAAG;AAChB,SAAO,QAAQ;AACf,MAAI;AACJ,SAAQ,OAAO,OAAO,KAAK,GAAI;AAC7B,aAAS,KAAK,IAAI,IAAI,KAAK;AAAA,EAC7B;AACA,SAAO;AACT;;;ADdO,IAAMC,SAAQ;AAAA,EACnB;AACF;;;AEPA;AAAA;AAAA;AAAA;AAAA;AAAA,eAAAC;AAAA;;;ACKO,IAAM,6BAAN,cAAyC,MAAM;AAAA,EACpD,YAAY,SAAwB,QAAqC;AACvE,UAAM,OAAO;AADqB;AAElC,SAAK,OAAO;AAAA,EACd;AACF;;;ACmBO,IAAK,8BAAL,kBAAKC,iCAAL;AACL,EAAAA,0DAAA,oBAAiB,OAAjB;AACA,EAAAA,0DAAA,eAAY,OAAZ;AACA,EAAAA,0DAAA,iBAAc,OAAd;AACA,EAAAA,0DAAA,2BAAwB,OAAxB;AAJU,SAAAA;AAAA,GAAA;AAOL,IAAK,+BAAL,kBAAKC,kCAAL;AACL,EAAAA,4DAAA,QAAK,OAAL;AADU,SAAAA;AAAA,GAAA;AAIL,IAAK,gCAAL,kBAAKC,mCAAL;AACL,EAAAA,8DAAA,eAAY,OAAZ;AADU,SAAAA;AAAA,GAAA;;;ACnCL,IAAM,kBAAkB,CAAC,SAAS,UAAU;;;ACSnD,IAAM,YAAY;AAClB,IAAM,WAAW;AACjB,IAAM,QAAQ,YAAY;AAC1B,IAAM,QAAQ;AACd,IAAM,OAAO;AACb,IAAM,QAAQ;AACd,IAAM,cAAc;AACpB,IAAM,eAAe;AACrB,IAAM,WAAW,eAAe;AAChC,IAAM,aAAa,QAAQ,QAAQ,OAAO;AAC1C,IAAM,cAAc,KAAK;AACzB,IAAM,QAAQ,UAAU,mBAAmB;AAC3C,IAAM,QAAQ,UAAU,aAAa,iBAAiB;AAEtD,IAAM,cAAc;AACpB,IAAM,cAAc;AACpB,IAAM,oBAAoB,GAAG,cAAc;AAC3C,IAAM,cAAc,SAAS,WAAW;AACxC,IAAM,gBAAgB,MAAM;AAgCrB,SAAS,SAAS,KAAa,SAAqC;AACzE,MAAI;AAKF,UAAM,qBAAqB,YAAY,GAAG;AAG1C,UAAM,iBAAiB,wBAAwB,kBAAkB;AAEjE,UAAM,YAAY,eAAe,eAAe,WAAW,OAAO;AAElE,WAAO;AAAA,MACL,GAAG;AAAA,MACH,KAAK,eAAe,IAAI,YAAY;AAAA,MACpC;AAAA,IACF;AAAA,EACF,SAAS,KAAP;AAEA,UAAM,IAAI;AAAA,MACR,+BAA+B,yBAAyB,IAAI;AAAA,IAC9D;AAAA,EACF;AACF;AAUO,SAAS,YAAY,KAAqB;AAC/C,QAAM,QAAQ,IAAI,OAAO,gBAAkB,oBAAoB;AAC/D,QAAM,UAAU,MAAM,KAAK,GAAG;AAG9B,MAAI,CAAC,SAAS;AACZ,UAAM,IAAI;AAAA,MACR,gEAAgE,MAAM,SAAS;AAAA,IACjF;AAAA,EACF;AAEA,SAAO,QAAQ,CAAC;AAClB;AASO,SAAS,wBACd,SACwB;AACxB,QAAM,eAAe,KAAK;AAC1B,QAAM,SAAS,IAAI;AACnB,QAAM,UAAU,GAAG,qBAAqB;AACxC,QAAM,WAAW,OAAO;AACxB,QAAM,UAAU,KAAK;AAIrB,QAAM,QAAQ,IAAI;AAAA,IAChB,OAAO,kBAAkB,iBAAiB,gBAAgB,gBAAgB;AAAA,EAC5E;AAEA,QAAM,MAAM,MAAM,KAAK,OAAO;AAE9B,MAAI,CAAC,KAAK;AACR,UAAM,IAAI;AAAA,MACR,2DAA2D,kDAAkD,MAAM,SAAS;AAAA,IAC9H;AAAA,EACF;AAEA,QAAM,CAAC,GAAG,WAAW,KAAK,MAAM,OAAO,QAAQ,IAAI;AAEnD,SAAO;AAAA,IACL;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,EACF;AACF;AAEA,IAAM,6BAAoE;AAAA,EACxE,OAAO,MACL,IAAI;AAAA,IACF,sBAAsB;AAAA,EACxB;AAAA,EACF,UAAU,MACR,IAAI,OAAO,SAAS,qDAAqD;AAC7E;AAKA,IAAM,yBAAiD;AAAA,EACrD,OAAO,CAAC,SAAS,OAAO,KAAK;AAAA,EAC7B,UAAU,CAAC,YAAY,KAAK;AAC9B;AAKA,IAAM,2BAAqD;AAAA,EACzD,OAAO;AAAA,EACP,UAAU;AACZ;AA6BO,SAAS,eACd,WACA,SACc;AAEd,MAAI,MAA6B,EAAE,GAAG,QAAQ;AAE9C,MAAI,WAAW;AAGb,QAAI,OAAe;AACnB,eAAW,QAAQ,iBAAiB;AAElC,cAAQ,2BAA2B,IAAI,EAAE;AACzC,YAAM,MAAM,KAAK,SAAS;AAE1B,UAAI,CAAC;AAAK;AAEV,YAAM,CAAC,UAAU,gBAAgB,YAAY,IAAI,IAAI,OAAO,GAAG,CAAC;AAChE,mBAAa,IAAI,WAAW;AAC5B,yBAAmB,IAAI,iBAAiB;AACxC,uBAAiB,IAAI,eAAe;AACpC;AAAA,IACF;AAAA,EACF;AAIA,MAAI,CAAC,IAAI,gBAAgB;AACvB,UAAM,IAAI;AAAA,MACR;AAAA,IACF;AAAA,EACF;AAEA,aAAW,CAAC,MAAM,MAAM,KAAK,OAAO,QAAQ,sBAAsB,GAAG;AACnE,QAAI,OAAO,SAAS,IAAI,cAAc,GAAG;AACvC,UAAI,iBAAiB;AACrB;AAAA,IACF;AAAA,EACF;AAIA,MAAI,CAAC,IAAI,cAAc;AACrB,QAAI,eAAe,yBAAyB,IAAI,cAAc;AAAA,EAChE;AAGA,MAAI,CAAC,IAAI,cAAc;AACrB,UAAM,IAAI;AAAA,MACR;AAAA,IACF;AAAA,EACF;AAGA,MAAI,CAAC,IAAI,UAAU;AACjB,QAAI,WACF,kBAAkB,GAAG,IAAI,kBAAkB,IAAI,cAAc;AAAA,EACjE;AAGA,MAAI,CAAC,IAAI,UAAU;AACjB,UAAM,IAAI;AAAA,MACR;AAAA,IACF;AAAA,EACF;AAEA,SAAO;AACT;;;AC1PA,IAAM,mBAAgE;AAAA,EACpE,sBAAwC,GAAG;AAAA,EAC3C,yBAA2C,GAAG;AAAA,EAC9C,oBAAsC,GAAG;AAAA,EACzC,gCAAkD,GAAG;AACvD;AAkBO,SAAS,oBAAoB,UAAoB;AAStD,SAAO,OAAO,QAAkD;AAC9D,QAAI;AACF,UAAI,IAAI,WAAW,GAAG,GAAG;AACvB,cAAM,IAAI,MAAM,CAAC;AAAA,MACnB;AAEA,UAAI;AACJ,UAAI;AAGF,YAAI,IAAI,WAAW,WAAW,GAAG;AAC/B,gBAAM,YAAY,GAAG;AAAA,QACvB;AAEA,qBAAa,wBAAwB,GAAG;AAAA,MAC1C,SAAS,KAAP;AACA,cAAM,IAAI;AAAA,UACR,8BAA8B,IAAI;AAAA;AAAA,QAEpC;AAAA,MACF;AAEA,UAAI,EAAE,KAAK,MAAM,UAAU,IAAI;AAE/B,UAAI,QAAQ,MAAM,MAAM,GAAG,KAAK,CAAC;AACjC,cAAQ,MAAM,OAAO,QAAM,GAAG,SAAS,CAAC;AAGxC,UAAI;AACJ,UAAI,WAAW;AACb,0BAAkB,eAAe,SAAS;AAAA,MAC5C;AAGA,UAAI;AACJ,UAAI;AACJ,UAAI;AACF,gBAAQ,YAAY,MAAM,SAAS;AAAA,UACjC;AAAA,UACA;AAAA,UACA;AAAA,QACF;AACA,YAAI,CAAC,OAAO;AACV,gBAAM,IAAI;AAAA,YACR,gCAAgC,QAAQ;AAAA;AAAA,UAE1C;AAAA,QACF;AAAA,MACF,SAAS,KAAP;AAGA,YAAI,eAAe,4BAA4B;AAC7C,gBAAM;AAAA,QACR,OAAO;AACL,gBAAM,IAAI;AAAA,YACR,IAAI;AAAA;AAAA,UAEN;AAAA,QACF;AAAA,MACF;AAGA,UAAK,MAAiC,OAAO;AAC3C,YAAK,MAAiC,MAAM,YAAY,GAAG;AACzD,cAAI;AACF,oBAAQ,MAAM,SAAS;AAAA,cACrB,MAAM;AAAA,cACN,CAAC,YAAY;AAAA,cACb;AAAA,YACF;AAAA,UACF,SAAS,KAAP;AAEA,kBAAM,IAAI;AAAA,cACR,wFACE,MAAM,MACL,IAAI,UAAU,KAAK,IAAI,YAAY;AAAA;AAAA,YAExC;AAAA,UACF;AAAA,QACF,OAEK;AACH,gBAAM,IAAI;AAAA,YACR,iBAAiB,QAAQ;AAAA;AAAA,UAE3B;AAAA,QACF;AAAA,MACF;AAGA,UAAK,MAAiC,SAAS,CAAC,OAAO;AACrD,cAAM,IAAI;AAAA,UACR,mCAAmC,QAAQ;AAAA;AAAA,QAE7C;AAAA,MACF;AAGA,UAAI;AACJ,UAAI;AACF,cAAM,eAAe,MAAM,SAAS;AAAA,UAClC,MAAM;AAAA,UACL,MAA4B;AAAA,UAC7B;AAAA,QACF;AACA,kBACE,OAAO,iBAAiB,WACpB,iBAAiB,YAAY,IAC7B;AAAA,MACR,SAAS,KAAP;AACA,cAAM,IAAI;AAAA,UACR,iEACE,MAAM,MACL,IAAI,UAAU,KAAK,IAAI,YAAY;AAAA;AAAA,QAExC;AAAA,MACF;AAIA,UAAI;AACJ,YAAM,mBAAoB,MAA4B;AACtD,UAAI;AACF,cAAM,cACJ,OAAO,qBAAqB,WACxB,iBAAiB,gBAAgB,IACjC;AACN,kBAAU,eAAe,WAAW;AAAA,MACtC,SAAS,KAAP;AACA,cAAM,IAAI;AAAA,UACR,kEACE,MAAM,4BACiB,oBACvB,IAAI,UAAU,KAAK,IAAI,YAAY;AAAA;AAAA,QAGvC;AAAA,MACF;AAEA,UAAI;AASJ,YAAM,eACJ,WAAW,OAAO,WAAW,OAAO,IAAI,WAAW,SAAS;AAC9D,UACG,UAAqC,SACtC,CAAC,aAAa,SAAS,GAAG;AAAA;AAAA;AAAA;AAAA,MAK1B,EAAE,WAAW,SAAS,KACtB;AACA,cAAM,WACJ,MACA,WAAW,OACV,WAAW,OAAO,IAAI,WAAW,SAAS,MAC3C,OACC,WAAW,QAAQ,IAAI,WAAW,UAAU;AAE/C,kBAAU;AAAA;AAAA,UAER,GAAG;AAAA,UACH,UAAU;AAAA,QACZ;AACA;AAAA,MACF;AAGA,aAAO;AAAA,QACL;AAAA,QACA;AAAA,QACA;AAAA,MACF;AAAA,IACF,SAAS,KAAP;AAIA,UAAI,QAAqC;AACzC,UAAI,eAAe,4BAA4B;AAC7C,iBAAS,IAAI;AACb,gBAAQ;AAAA,UACN,MAAM,IAAI;AAAA,UACV,MAAM,iBAAiB,IAAI,MAAM;AAAA,UACjC,SAAS,IAAI;AAAA,QACf;AAAA,MACF,OAAO;AACL;AACA,gBAAQ;AAAA,UACN;AAAA,UACA,MAAM,gDAEN;AAAA,QACF;AAAA,MACF;AAEA,aAAO;AAAA,QACL;AAAA;AAAA;AAAA,QAGA,SAAS,IAAI,YAAY,EAAE;AAAA,UACzB,OAAO,UAAU,MAAM,YACrB,MAAM,UAAU,MAAM,IAAI,gBAAgB;AAAA,QAE9C;AAAA;AAAA,QAEA,SAAS;AAAA,UACP,gBAAgB;AAAA,QAClB;AAAA,QACA;AAAA,MACF;AAAA,IACF;AAAA,EACF;AACF;;;AClRA;AAAA;AAAA;AAAA,eAAAC;AAAA;AAIO,IAAMC,SAAQ;AAAA,EACnB;AAAA,EACA;AAAA,EACA;AACF;;;ACDA,IAAM,SAAS;AAAA,EACb;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AACF;AAEA,IAAO,cAAQ;AAMf,IAAI,OAAO,WAAW,aAAa;AACjC;AAAC,EAAC,OAAe,SAAS;AAC5B;","names":["hpack","hpack","part","utils","hpack","utils","utils","ProxyResolutionStatusErrors","ProxyResolutionStatusSuccess","ProxyResolutionStatusRedirect","utils","utils"]}