{"version":3,"sources":["../src/config.ts","../src/utils/keccak.ts","../src/utils/string.ts","../src/utils/uint8.ts","../src/files/chunks.ts","../src/files/inscriptions.ts","../src/files/file.ts","../src/files/metadata.ts","../src/files/directory.ts","../src/resolve/uri.ts","../src/resolve/errors.ts","../src/resolve/proxy.ts","../src/files/index.ts","../src/index.ts"],"names":["part","uri","resolver"],"mappings":";;;;;;;AAEO,IAAM,wBAAwB;AAAA,EACnC,MAAM,IAAI,WAAW,CAAC,CAAC,CAAC;AAAA,EACxB,WAAW,IAAI,WAAW,CAAC,CAAC,CAAC;AAC/B;AAKO,IAAM,qBAAqB;;;ACVlC,SAAS,iBAAiB;AAMnB,SAAS,OAAO,OAAwC;AAC7D,SAAO,IAAI,WAAW,UAAU,OAAO,KAAK,CAAC;AAC/C;;;ACDO,SAAS,iBAAiB,KAAyB;AACxD,QAAM,MAAM,IAAI,OAAO,uBAAuB;AAC9C,MAAI,CAAC,IAAI,KAAK,GAAG,GAAG;AAClB,UAAM,IAAI;AAAA,MACR;AAAA,YAAiF,IAAI,SAAS,CAAC;AAAA,MAAS,IAAI;AAAA,QAC1G;AAAA,QACA;AAAA,MACF,CAAC,GAAG,IAAI,SAAS,KAAK,QAAQ,EAAE;AAAA,IAClC;AAAA,EACF;AACA,QAAM,QAAQ,IAAI,WAAW,IAAI,SAAS,CAAC;AAC3C,WAAS,IAAI,GAAG,IAAI,IAAI,SAAS,GAAG,KAAK;AACvC,UAAM,CAAC,IAAI,SAAS,IAAI,MAAM,IAAI,GAAG,IAAI,IAAI,CAAC,GAAG,EAAE;AAAA,EACrD;AACA,SAAO;AACT;;;ACbO,SAAS,gBACd,QACA,SAAS,GACT,QACA;AACA,WAAS,OAAO,WAAW,cAAc,OAAO,aAAa,SAAS;AACtE,QAAM,MAAM,IAAI,WAAW,MAAM;AACjC,WAAS,IAAI,GAAG,IAAI,QAAQ,KAAK;AAC/B,QAAI,CAAC,IAAI,OAAO,IAAI,MAAM;AAAA,EAC5B;AACA,SAAO;AACT;AAQO,SAAS,qBAAqB,QAAkC;AACrE,QAAM,IAAI,OAAO,OAAO,CAAC,KAAK,QAAQ,IAAI,SAAS,KAAK,CAAC;AACzD,QAAM,MAAM,IAAI,WAAW,CAAC;AAC5B,MAAI,SAAS;AACb,aAAW,OAAO,QAAQ;AACxB,QAAI,IAAI,KAAK,MAAM;AACnB,cAAU,IAAI;AAAA,EAChB;AACA,SAAO;AACT;AASO,SAAS,mBAAmB,GAAe,GAAuB;AAEvE,WAAS,IAAI,GAAG,IAAI,EAAE,QAAQ,KAAK;AACjC,QAAI,EAAE,CAAC,IAAI,EAAE,CAAC;AAAG,aAAO;AACxB,QAAI,EAAE,CAAC,IAAI,EAAE,CAAC;AAAG,aAAO;AAAA,EAC1B;AACA,SAAO;AACT;AAUO,SAAS,mBAAmB,GAAe,GAAwB;AACxE,MAAI,EAAE,WAAW,EAAE;AAAQ,WAAO;AAClC,WAAS,IAAI,GAAG,IAAI,EAAE,QAAQ,KAAK;AACjC,QAAI,EAAE,CAAC,MAAM,EAAE,CAAC;AAAG,aAAO;AAAA,EAC5B;AACA,SAAO;AACT;;;AClDO,SAAS,WACd,SACA,YAAoB,oBACP;AACb,MAAI,aAAa,GAAG;AAClB,UAAM,IAAI,MAAM,8CAA8C;AAAA,EAChE;AACA,QAAM,IAAI,QAAQ;AAClB,QAAM,KAAK,KAAK,KAAK,IAAI,SAAS;AAClC,QAAM,SAAsB,CAAC;AAC7B,MAAI;AACJ,WAAS,IAAI,GAAG,IAAI,IAAI,KAAK;AAC3B,YAAQ;AAAA,MACN;AAAA,MACA,IAAI;AAAA,MACJ,KAAK,IAAI,WAAW,IAAI,IAAI,SAAS;AAAA,IACvC;AACA,WAAO,KAAK;AAAA,MACV,OAAO;AAAA,MACP,MAAM,OAAO,KAAK;AAAA,IACpB,CAAC;AAAA,EACH;AACA,SAAO;AACT;;;ACRO,SAAS,qBAAqB,MAA4B;AAC/D,QAAM,eAA8B,CAAC;AACrC,QAAM,WAAW,CAAC,SAAgB;AAChC,QAAI,KAAK,SAAS,aAAa;AAC7B,mBAAa,KAAK;AAAA,QAChB,MAAM;AAAA,QACN,OAAO,OAAO;AAAA,UACZ,OAAO,KAAK,KAAK,KAAK,EAAE,IAAI,UAAQ,CAAC,MAAM,KAAK,MAAM,IAAI,EAAE,GAAG,CAAC;AAAA,QAClE;AAAA,MACF,CAAC;AAED,iBAAW,QAAQ,KAAK,OAAO;AAC7B,iBAAS,KAAK,MAAM,IAAI,CAAC;AAAA,MAC3B;AAAA,IACF,WAAW,KAAK,SAAS,QAAQ;AAG/B,mBAAa,KAAK;AAAA,QAChB,MAAM;AAAA,QACN,QAAQ,KAAK,OAAO,IAAI,SAAO,IAAI,IAAI;AAAA,QACvC,UAAU,KAAK;AAAA,MACjB,CAAC;AACD,iBAAW,SAAS,KAAK,QAAQ;AAC/B,qBAAa,KAAK;AAAA,UAChB,MAAM;AAAA,UACN,SAAS,MAAM;AAAA,QACjB,CAAC;AAAA,MACH;AAAA,IACF;AAAA,EACF;AACA,WAAS,IAAI;AACb,SAAO,aAAa,QAAQ;AAC9B;AAOA,SAAS,wBAAwB,KAAkB;AACjD,UAAQ,IAAI,MAAM;AAAA,IAChB,KAAK;AAEH,aAAO,IAAI,QAAQ,aAAa;AAAA,IAClC,KAAK;AAKH,aACE,OAAO,KAAK,IAAI,KAAK,EAClB,IAAI,UAAQ,KAAK,SAAS,EAAE,EAC5B,OAAO,CAAC,GAAG,MAAM,IAAI,GAAG,CAAC,IAAI;AAAA,IAEpC,KAAK;AAEH,aACE;AAAA,MACA,KAAK,IAAI,OAAO;AAAA,MAChB,IAAI,SAAS,IAAI,SAAO,IAAI,UAAU,EAAE,OAAO,CAAC,GAAG,MAAM,IAAI,GAAG,CAAC;AAAA,EAEvE;AACF;AAWO,SAAS,yBAAyB,cAA6B;AACpE,SAAO,aAAa;AAAA,IAClB,CAAC,KAAK,QAAQ,wBAAwB,GAAG,IAAI;AAAA,IAC7C;AAAA,EACF;AACF;;;AChHA,SAAS,YAAY;;;ACkBd,IAAM,wBAA+C;AAAA,EAC1D,gBAAgB,IAAI,WAAW,CAAC,GAAG,CAAC,CAAC;AAAA,EACrC,oBAAoB,IAAI,WAAW,CAAC,GAAG,CAAC,CAAC;AAC3C;AAIO,IAAM,2BAA2B;AAAA,EACtC;AAAA;AACF;AAQO,SAAS,sBAAsB,OAAqB;AACzD,WAAS,IAAI,GAAG,IAAI,MAAM,QAAQ,KAAK;AACrC,QAAI,yBAAyB,SAAS,MAAM,WAAW,CAAC,CAAC,GAAG;AAC1D,YAAM,IAAI;AAAA,QACR,qCAAqC,MAAM;AAAA,UACzC;AAAA,QACF,CAAC,iBAAiB,CAAC;AAAA,MACrB;AAAA,IACF;AAAA,EACF;AACF;AAUO,SAAS,mBACd,UACc;AACd,QAAM,MAAoB,CAAC;AAC3B,MAAI;AACJ,aAAW,SAAS,UAAU;AAC5B,QAAI,sBAAsB,KAAK,GAAG;AAEhC,cAAQ,SAAS,KAAK;AACtB,UAAI;AACF,8BAAsB,KAAK;AAAA,MAC7B,SAAS,KAAU;AACjB,cAAM,IAAI;AAAA,UACR,6CAA6C,KAAK,MAAM,IAAI,OAAO;AAAA,QACrE;AAAA,MACF;AACA,UAAI;AAAA,QACF;AAAA,UACE,sBAAsB,KAAK;AAAA,UAC3B,IAAI,YAAY,EAAE,OAAO,KAAK;AAAA,QAChC;AAAA,MACF;AAAA,IACF;AAAA,EACF;AACA,MAAI,KAAK,kBAAkB;AAC3B,SAAO;AACT;AAEO,SAAS,oBAAoB,OAAmB;AACrD,QAAM,gBAAgB,gBAAgB,OAAO,GAAG,CAAC;AACjD,aAAW,CAAC,QAAQ,QAAQ,KAAK,OAAO,QAAQ,qBAAqB,GAAG;AACtE,QAAI,mBAAmB,eAAe,QAAQ,GAAG;AAC/C,aAAO,CAAC,QAAQ,IAAI,YAAY,EAAE,OAAO,gBAAgB,OAAO,CAAC,CAAC,CAAC;AAAA,IACrE;AAAA,EACF;AACA,SAAO;AACT;AAEO,SAAS,mBAAmB,KAAmB;AACpD,QAAM,WAAgC,CAAC;AACvC,aAAW,QAAQ,KAAK;AACtB,UAAM,QAAQ,oBAAoB,IAAI;AACtC,QAAI,OAAO;AACT,eAAS,MAAM,CAAC,CAAC,IAAI,MAAM,CAAC;AAAA,IAC9B;AAAA,EACF;AACA,SAAO;AACT;;;ADlGA,SAAS,UAAU,kBAAkB;AAoBrC,eAAsB,YACpB,MACA,SACA,YAAoB,oBACA;AACpB,MAAI,WAAgC,CAAC;AACrC,MAAI,iBAAiB;AAErB,MAAI,OAAO,WAAW,IAAI;AAE1B,MAAI,CAAC,MAAM;AAAA,EAUX,OAAO;AACL,aAAS,cAAc,IAAI;AAAA,EAC7B;AAGA,QAAM,aAAa,KAAK,OAAO;AAC/B,MAAI,WAAW,aAAa,eAAe,YAAY;AACrD,qBAAiB;AACjB,aAAS,kBAAkB,IAAI;AAAA,EACjC;AAGA,QAAM,SAAS,WAAW,gBAAgB,SAAS;AAEnD,QAAM,kBAAkB,mBAAmB,QAAQ;AAGnD,QAAM,cAAc,OAAO,cAAc;AACzC,QAAM,eAAe,OAAO,kBAAkB,GAAG,eAAe,CAAC;AACjE,QAAM,MAAM;AAAA,IACV,kBAAkB,sBAAsB,MAAM,aAAa,YAAY;AAAA,EACzE;AAEA,SAAO;AAAA,IACL,MAAM;AAAA,IACN;AAAA,IACA;AAAA,IACA,UAAU;AAAA,EACZ;AACF;;;AErDO,SAAS,eAAe,MAAsB;AACnD,SAAO,mBAAmB,IAAI;AAChC;AAQO,SAAS,sBACd,KACgB;AAChB,QAAM,MAAoB,CAAC;AAC3B,QAAM,YAAY,OAAO,KAAK,IAAI,KAAK,EAAE,KAAK;AAC9C,QAAM,WAAkC,CAAC;AACzC,aAAW,YAAY,WAAW;AAChC,UAAM,QAAQ,IAAI,MAAM,QAAQ,EAAE;AAClC,aAAS,QAAQ,IAAI;AAErB,QAAI,QAAQ,OAAO,QAAQ,CAAC;AAE5B,QAAI,QAAQ,MAAM,GAAG;AAAA,EACvB;AAEA,MAAI,QAAQ,sBAAsB,SAAS;AAC3C,SAAO;AAAA,IACL,MAAM;AAAA,IACN,KAAK,OAAO,kBAAkB,GAAG,GAAG,CAAC;AAAA,IACrC,OAAO;AAAA,EACT;AACF;AAWO,SAAS,oBACd,OAC+C;AAC/C,MAAI,QAA6B;AAAA,IAC/B,MAAM;AAAA,IACN,OAAO,CAAC;AAAA,IACR,QAAQ;AAAA,EACV;AACA,QAAM,SAAiC,CAAC;AAExC,aAAW,QAAQ,OAAO;AACxB,QAAI,SAAS,OACX,OAAe;AACjB,UAAM,gBAAgB,KAAK,KAAK,WAAW,IAAI,IAC3C,KAAK,KAAK,MAAM,CAAC,IACjB,KAAK;AAIT,UAAM,QAAQ,cAAc,MAAM,GAAG,EAAE,IAAI,CAAAA,UAAQ,eAAeA,KAAI,CAAC;AACvE,aAAS,IAAI,GAAG,IAAI,MAAM,QAAQ,KAAK;AACrC,aAAO,MAAM,CAAC;AAEd,UAAI,KAAK,WAAW,GAAG;AACrB,cAAM,IAAI;AAAA,UACR,YAAY,KAAK,IAAI;AAAA,QACvB;AAAA,MACF;AAEA,UAAI,MAAM,MAAM,SAAS,GAAG;AAG1B,YAAI,OAAO,MAAM,eAAe,IAAI,GAAG;AACrC,gBAAM,IAAI;AAAA,YACR,oBAAoB,KAAK,IAAI;AAAA,UAC/B;AAAA,QACF;AACA,cAAM,SAA+B;AAAA,UACnC,MAAM;AAAA,UACN,SAAS,KAAK;AAAA,UACd,MAAM;AAAA,UACN,QAAQ;AAAA,QACV;AACA,eAAO,MAAM,IAAI,IAAI;AACrB,eAAO,KAAK,MAAM;AAAA,MACpB,OAEK;AACH,YAAI,OAAO,MAAM,eAAe,IAAI,GAAG;AACrC,mBAAS,OAAO,MAAM,IAAI;AAAA,QAC5B,OAAO;AACL,gBAAM,OAA4B;AAAA,YAChC,MAAM;AAAA,YACN,OAAO,CAAC;AAAA,YACR,QAAQ;AAAA,UACV;AACA,iBAAO,MAAM,IAAI,IAAI;AACrB,mBAAS;AAAA,QACX;AAAA,MACF;AAAA,IACF;AAAA,EACF;AAEA,SAAO,CAAC,OAAO,MAAM;AACvB;AAcA,eAAsB,iBACpB,OACA,YAAoB,oBACK;AACzB,QAAM,CAAC,OAAO,MAAM,IAAI,oBAAoB,KAAK;AAEjD,QAAM,SAAiC,CAAC;AACxC,MAAI,UAAkC;AAEtC,SAAO,QAAQ,SAAS,GAAG;AACzB,UAAM,YAAoC,CAAC;AAC3C,eAAW,QAAQ,SAAS;AAE1B,UAAI,OAAO,SAAS,IAAI;AAAG;AAC3B,UAAI,KAAK,SAAS,QAAQ;AACxB,aAAK,QAAQ,MAAM,YAAY,KAAK,MAAM,KAAK,SAAS,SAAS;AAAA,MACnE,WAAW,KAAK,SAAS,aAAa;AAEpC,aAAK,QAAQ,sBAAsB,IAAI;AAAA,MACzC;AAEA,aAAO,KAAK,IAAI;AAGhB,UAAI,KAAK,QAAQ;AAGf,cAAM,WAAW,OAAO,OAAO,KAAK,OAAO,KAAK;AAChD,YAAI,CAAC,SAAS,KAAK,WAAS,CAAC,MAAM,KAAK,GAAG;AACzC,oBAAU,KAAK,KAAK,MAAM;AAAA,QAC5B;AAAA,MACF;AAAA,IACF;AAEA,cAAU;AAAA,EACZ;AAIA,SAAO,MAAM;AACf;;;AC/KO,IAAM,kBAAkB,CAAC,SAAS,UAAU;AAwC5C,IAAM,sBAA8C;AAAA,EACzD,iBAAiB;AAAA,EACjB,kBAAkB;AAAA,EAClB,cAAc;AAAA,EACd,cAAc;AAChB;AAMA,IAAM,YAAY;AAClB,IAAM,WAAW;AACjB,IAAM,QAAQ,YAAY;AAC1B,IAAM,QAAQ;AACd,IAAM,OAAO;AACb,IAAM,QAAQ;AACd,IAAM,cAAc;AACpB,IAAM,eAAe;AACrB,IAAM,WAAW,eAAe;AAChC,IAAM,aAAa,QAAQ,QAAQ,OAAO;AAC1C,IAAM,cAAc,KAAK,WAAW;AACpC,IAAM,QAAQ,UAAU,UAAU,SAAS,WAAW;AACtD,IAAM,QAAQ,UAAU,UAAU,GAAG,QAAQ,SAAS,WAAW;AAEjE,IAAM,cAAc;AACpB,IAAM,cAAc;AACpB,IAAM,oBAAoB,GAAG,WAAW,GAAG,WAAW;AACtD,IAAM,cAAc,SAAS,KAAK,MAAM,YAAY;AACpD,IAAM,gBAAgB,MAAM,WAAW;AAgChC,SAAS,SAASC,MAAa,SAAqC;AACzE,MAAI;AAKF,UAAM,qBAAqB,YAAYA,IAAG;AAG1C,UAAM,iBAAiB,wBAAwB,kBAAkB;AAEjE,UAAM,YAAY,eAAe,eAAe,WAAW,OAAO;AAElE,WAAO;AAAA,MACL,GAAG;AAAA,MACH,KAAK,eAAe,IAAI,YAAY;AAAA,MACpC;AAAA,IACF;AAAA,EACF,SAAS,KAAU;AAEjB,UAAM,IAAI;AAAA,MACR,+BAA+BA,IAAG,sBAAsB,IAAI,OAAO;AAAA,IACrE;AAAA,EACF;AACF;AAUO,SAAS,YAAYA,MAAqB;AAC/C,QAAM,QAAQ,IAAI,OAAO,gBAAkB,WAAW,SAAS;AAC/D,QAAM,UAAU,MAAM,KAAKA,IAAG;AAG9B,MAAI,CAAC,SAAS;AACZ,UAAM,IAAI;AAAA,MACR,gEAAgE,MAAM,SAAS,CAAC;AAAA,IAClF;AAAA,EACF;AAEA,SAAO,QAAQ,CAAC;AAClB;AAqBO,SAAS,wBACd,SACwB;AACxB,QAAM,eAAe,KAAK,iBAAiB;AAC3C,QAAM,SAAS,IAAI,WAAW;AAC9B,QAAM,UAAU,GAAG,WAAW,UAAU,WAAW;AACnD,QAAM,WAAW,OAAO,aAAa;AACrC,QAAM,UAAU,KAAK,aAAa;AAIlC,QAAM,QAAQ,IAAI;AAAA,IAChB,OAAO,YAAY,MAAM,MAAM,WAAW,OAAO,SAAS,QAAQ,QAAQ,OAAO;AAAA,EACnF;AAEA,QAAM,MAAM,MAAM,KAAK,OAAO;AAE9B,MAAI,CAAC,KAAK;AACR,UAAM,IAAI;AAAA,MACR,2DAA2D,OAAO,2CAA2C,MAAM,SAAS,CAAC;AAAA,IAC/H;AAAA,EACF;AAEA,QAAM,CAAC,GAAG,WAAW,KAAK,MAAM,OAAO,QAAQ,IAAI;AAEnD,SAAO;AAAA,IACL;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,EACF;AACF;AAEA,IAAM,6BAAoE;AAAA,EACxE,OAAO,MACL,IAAI;AAAA,IACF,sBAAsB,WAAW;AAAA,EACnC;AAAA,EACF,UAAU,MACR,IAAI,OAAO,SAAS,WAAW,0CAA0C;AAC7E;AAKA,IAAM,yBAAiD;AAAA,EACrD,OAAO,CAAC,SAAS,OAAO,KAAK;AAAA,EAC7B,UAAU,CAAC,YAAY,KAAK;AAC9B;AAKA,IAAM,2BAAqD;AAAA,EACzD,OAAO;AAAA,EACP,UAAU;AACZ;AA6BO,SAAS,eACd,WACA,SACc;AAEd,MAAI,MAA6B,EAAE,GAAG,QAAQ;AAE9C,MAAI,WAAW;AAGb,QAAI,OAAe;AACnB,eAAW,QAAQ,iBAAiB;AAElC,cAAQ,2BAA2B,IAAI,EAAE;AACzC,YAAM,MAAM,KAAK,SAAS;AAE1B,UAAI,CAAC;AAAK;AAEV,YAAM,CAAC,UAAU,gBAAgB,YAAY,IAAI,IAAI,OAAO,GAAG,CAAC;AAChE,mBAAa,IAAI,WAAW;AAC5B,yBAAmB,IAAI,iBAAiB;AACxC,uBAAiB,IAAI,eAAe;AACpC;AAAA,IACF;AAAA,EACF;AAIA,MAAI,CAAC,IAAI,gBAAgB;AACvB,UAAM,IAAI;AAAA,MACR;AAAA,IACF;AAAA,EACF;AAEA,aAAW,CAAC,MAAM,MAAM,KAAK,OAAO,QAAQ,sBAAsB,GAAG;AACnE,QAAI,OAAO,SAAS,IAAI,cAAc,GAAG;AACvC,UAAI,iBAAiB;AACrB;AAAA,IACF;AAAA,EACF;AAIA,MAAI,CAAC,IAAI,cAAc;AACrB,QAAI,eAAe,yBAAyB,IAAI,cAAc;AAAA,EAChE;AAGA,MAAI,CAAC,IAAI,cAAc;AACrB,UAAM,IAAI;AAAA,MACR;AAAA,IACF;AAAA,EACF;AAGA,MAAI,CAAC,IAAI,UAAU;AACjB,QAAI,WACF,oBAAoB,GAAG,IAAI,cAAc,IAAI,IAAI,YAAY,EAAE;AAAA,EACnE;AAGA,MAAI,CAAC,IAAI,UAAU;AACjB,UAAM,IAAI;AAAA,MACR;AAAA,IACF;AAAA,EACF;AAEA,SAAO;AACT;;;AClUO,IAAM,6BAAN,cAAyC,MAAM;AAAA,EACpD,YAAY,SAAwB,QAAqC;AACvE,UAAM,OAAO;AADqB;AAElC,SAAK,OAAO;AAAA,EACd;AACF;;;ACkCA,IAAM,mBAAgE;AAAA,EACpE,CAAC,qBAAuC,GAAG;AAAA,EAC3C,CAAC,wBAA0C,GAAG;AAAA,EAC9C,CAAC,mBAAqC,GAAG;AAAA,EACzC,CAAC,+BAAiD,GAAG;AACvD;AAkDO,SAAS,oBAAoBC,WAAoB;AAStD,SAAO,OAAOD,SAAkD;AAC9D,QAAI;AACF,UAAIA,KAAI,WAAW,GAAG,GAAG;AACvB,QAAAA,OAAMA,KAAI,MAAM,CAAC;AAAA,MACnB;AAEA,UAAI;AACJ,UAAI;AAGF,YAAIA,KAAI,WAAW,WAAW,GAAG;AAC/B,UAAAA,OAAM,YAAYA,IAAG;AAAA,QACvB;AAEA,qBAAa,wBAAwBA,IAAG;AAAA,MAC1C,SAAS,KAAK;AACZ,cAAM,IAAI;AAAA,UACR,8BAA8B,IAAI,OAAO;AAAA,UACzC;AAAA,QACF;AAAA,MACF;AAEA,UAAI,EAAE,KAAK,MAAM,UAAU,IAAI;AAE/B,UAAI,QAAQ,MAAM,MAAM,GAAG,KAAK,CAAC;AACjC,cAAQ,MAAM,OAAO,QAAM,GAAG,SAAS,CAAC;AAGxC,UAAI;AACJ,UAAI,WAAW;AACb,0BAAkB,eAAe,SAAS;AAAA,MAC5C;AAGA,UAAI;AACJ,UAAI;AACJ,UAAI;AACF,gBAAQ,YAAY,MAAMC,UAAS;AAAA,UACjC;AAAA,UACA;AAAA,UACA;AAAA,QACF;AACA,YAAI,CAAC,OAAO;AACV,gBAAM,IAAI;AAAA,YACR,gCAAgC,GAAG,KAAK,IAAI;AAAA,YAC5C;AAAA,UACF;AAAA,QACF;AAAA,MACF,SAAS,KAAK;AAGZ,YAAI,eAAe,4BAA4B;AAC7C,gBAAM;AAAA,QACR,OAAO;AACL,gBAAM,IAAI;AAAA,YACR,IAAI;AAAA,YACJ;AAAA,UACF;AAAA,QACF;AAAA,MACF;AAGA,UAAK,MAAiC,OAAO;AAC3C,YAAK,MAAiC,MAAM,YAAY,GAAG;AACzD,cAAI;AACF,oBAAQ,MAAMA,UAAS;AAAA,cACrB,MAAM;AAAA,cACN,CAAC,YAAY;AAAA,cACb;AAAA,YACF;AAAA,UACF,SAAS,KAAK;AAEZ,kBAAM,IAAI;AAAA,cACR,wFACE,MAAM,GACR,GAAG,IAAI,UAAU,KAAK,IAAI,OAAO,KAAK,EAAE;AAAA,cACxC;AAAA,YACF;AAAA,UACF;AAAA,QACF,OAEK;AACH,gBAAM,IAAI;AAAA,YACR,iBAAiB,GAAG,KAAK,IAAI;AAAA,YAC7B;AAAA,UACF;AAAA,QACF;AAAA,MACF;AAGA,UAAK,MAAiC,SAAS,CAAC,OAAO;AACrD,cAAM,IAAI;AAAA,UACR,mCAAmC,GAAG,KAAK,IAAI;AAAA,UAC/C;AAAA,QACF;AAAA,MACF;AAGA,UAAI;AACJ,UAAI;AACF,cAAM,eAAe,MAAMA,UAAS;AAAA,UAClC,MAAM;AAAA,UACL,MAA4B;AAAA,UAC7B;AAAA,QACF;AACA,kBACE,OAAO,iBAAiB,WACpB,iBAAiB,YAAY,IAC7B;AAAA,MACR,SAAS,KAAK;AACZ,cAAM,IAAI;AAAA,UACR,iEACE,MAAM,GACR,GAAG,IAAI,UAAU,KAAK,IAAI,OAAO,KAAK,EAAE;AAAA,UACxC;AAAA,QACF;AAAA,MACF;AAIA,UAAI;AACJ,YAAM,mBAAoB,MAA4B;AACtD,UAAI;AACF,cAAM,cAAc,iBAAiB;AAAA,UAAI,SACvC,OAAO,QAAQ,WAAW,iBAAiB,GAAG,IAAI;AAAA,QACpD;AACA,kBAAU,mBAAmB,WAAW;AAAA,MAC1C,SAAS,KAAK;AACZ,cAAM,IAAI;AAAA,UACR,kEACE,MAAM,GACR,yBAAyB,gBAAgB,IACvC,IAAI,UAAU,KAAK,IAAI,OAAO,KAAK,EACrC;AAAA,UACA;AAAA,QACF;AAAA,MACF;AAEA,UAAI,SAAgC;AASpC,YAAM,eACJ,WAAW,OAAO,WAAW,OAAO,IAAI,WAAW,IAAI,KAAK;AAC9D,UACG,UAAqC,SACtC,CAAC,aAAa,SAAS,GAAG;AAAA;AAAA;AAAA;AAAA,MAK1B,EAAE,WAAW,SAAS,KACtB;AACA,cAAM,WACJ,MACA,WAAW,OACV,WAAW,OAAO,IAAI,WAAW,IAAI,KAAK,MAC3C,OACC,WAAW,QAAQ,IAAI,WAAW,KAAK,KAAK;AAE/C,kBAAU;AAAA;AAAA,UAER,GAAG;AAAA,UACH,UAAU;AAAA,QACZ;AACA,iBAAS;AAAA,MACX;AAGA,aAAO;AAAA,QACL;AAAA,QACA;AAAA,QACA;AAAA,MACF;AAAA,IACF,SAAS,KAAK;AAIZ,UAAI,QAAqC;AACzC,UAAI,eAAe,4BAA4B;AAC7C,iBAAS,IAAI;AACb,gBAAQ;AAAA,UACN,MAAM,IAAI;AAAA,UACV,MAAM,iBAAiB,IAAI,MAAM;AAAA,UACjC,SAAS,IAAI;AAAA,QACf;AAAA,MACF,OAAO;AACL,iBAAS;AACT,gBAAQ;AAAA,UACN,MAAM;AAAA,UACN,MAAM,iBACJ,+BACF;AAAA,QACF;AAAA,MACF;AAEA,aAAO;AAAA,QACL;AAAA;AAAA;AAAA,QAGA,SAAS,IAAI,YAAY,EAAE;AAAA,UACzB,OAAO,MAAM,IAAI,MAAM,IAAI,QACzB,MAAM,UAAU,MAAM,IAAI,OAAO,SAAS,EAC5C;AAAA,QACF;AAAA;AAAA,QAEA,SAAS;AAAA,UACP,gBAAgB;AAAA,QAClB;AAAA,QACA;AAAA,MACF;AAAA,IACF;AAAA,EACF;AACF;;;ACtUA;AAAA;AAAA;AAAA;AAAA;AAOA,IAAO,gBAAQ;AAAA,EACb;AAAA,EACA;AAAA,EACA;AAAA,EACA,OAAO;AAAA,IACL;AAAA,IACA,WAAW;AAAA,MACT;AAAA,MACA,cAAkB;AAAA,MAClB,cAAkB;AAAA,IACpB;AAAA,IACA,cAAc;AAAA,MACZ,qBAAkC;AAAA,IACpC;AAAA,IACA,UAAU;AAAA,MACR,WAAoB;AAAA,MACpB,eAAwB;AAAA,MACxB,QAAiB;AAAA,MACjB,QAAiB;AAAA,IACnB;AAAA,EACF;AACF;;;ACUA,IAAM,QAAQ;AAAA,EACZ;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AACF;AAKA,IAAM,SAAS;AAAA,EACb;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AACF;AAKA,IAAM,MAAM;AAAA,EACV,OAAO;AAAA,EACP,OAAO;AAAA,IACL;AAAA,IACA;AAAA,IACA;AAAA,EACF;AACF;AAEA,IAAM,WAAW;AAAA,EACf,QAAQ;AACV;AAeA,IAAM,SAAS;AAAA,EACb;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AACF;AACA,IAAO,cAAQ;AAGf,IAAI,OAAO,WAAW,aAAa;AACjC;AAAC,EAAC,OAAe,SAAS;AAC5B","sourcesContent":["// an identifying byte is used when hasing files & directories, in order to\n// prevent eventual collisions at a high level\nexport const INODE_BYTE_IDENTIFIER = {\n  FILE: new Uint8Array([1]),\n  DIRECTORY: new Uint8Array([0]),\n}\n\n// sort of a mgic value, as it's impossible to have a single number to rule\n// them all; applications would have to pick the right chunk size here as to\n// improve storage being shared as much as possible depending on the use cases\nexport const DEFAULT_CHUNK_SIZE = 16384\n","import { keccak256 } from \"js-sha3\"\n\n/**\n * Hashes some bytes with keccak256. Simple typed wrapper to ease implementation\n * @param bytes Bytes to hash\n */\nexport function keccak(bytes: Uint8Array | string): Uint8Array {\n  return new Uint8Array(keccak256.digest(bytes))\n}\n","/**\n * Converts a hexadecimal string in a list of bytes, considering each pair\n * of characters in the string represents a byte. Will throw if the format\n * of the string is incorrect.\n * @param hex A hexadecimal string\n * @returns The bytes decoded from the hexadecimal string\n */\nexport function hexStringToBytes(hex: string): Uint8Array {\n  const reg = new RegExp(\"^(?:[a-fA-F0-9]{2})*$\")\n  if (!reg.exec(hex)) {\n    throw new Error(\n      `Cannot decode an hexadecimal string because its pattern is invalid\\nExpected: ${reg.toString()}\\nGot ${hex.slice(\n        0,\n        80\n      )}${hex.length > 80 ? \"...\" : \"\"}`\n    )\n  }\n  const bytes = new Uint8Array(hex.length / 2)\n  for (let i = 0; i < hex.length / 2; i++) {\n    bytes[i] = parseInt(hex.slice(i * 2, i * 2 + 2), 16)\n  }\n  return bytes\n}\n","/**\n * Intanciates a new Uint8Array in which the requested bytes from the source\n * are copied into. Inspired by nodejs Bytes.copyBytesFrom()\n * @param source The source to copy from\n * @param offset Offset in the source\n * @param length Number of bytes to copy after the offset. If undefined (def),\n * will copy everything after the offset.\n * @returns A new Uint8Array\n */\nexport function BytesCopiedFrom(\n  source: Uint8Array,\n  offset = 0,\n  length?: number\n) {\n  length = typeof length === \"undefined\" ? source.byteLength - offset : length\n  const out = new Uint8Array(length)\n  for (let i = 0; i < length; i++) {\n    out[i] = source[i + offset]\n  }\n  return out\n}\n\n/**\n * Instanciates a new Uint8Array and concatenates the given Uint8Arrays\n * together in the newly instanciated array.\n * @param arrays The Uint8Arrays to concatenate together\n * @returns A new Uint8Array\n */\nexport function concatUint8Arrays(...arrays: Uint8Array[]): Uint8Array {\n  const L = arrays.reduce((acc, arr) => arr.length + acc, 0)\n  const out = new Uint8Array(L)\n  let offset = 0\n  for (const arr of arrays) {\n    out.set(arr, offset)\n    offset += arr.length\n  }\n  return out\n}\n\n/**\n * Naïve Uint8Arrays comparaison for sorting. Loops through the bytes of array A\n * and compare their value against bytes of array B at the same index.\n * @param a First Uint8Array to compare\n * @param b Second Uint8Array to compare\n * @returns -1 if a < b, otherwise 1\n */\nexport function compareUint8Arrays(a: Uint8Array, b: Uint8Array): number {\n  // negative if a is less than b\n  for (let i = 0; i < a.length; i++) {\n    if (a[i] < b[i]) return -1\n    if (a[i] > b[i]) return 1\n  }\n  return 1\n}\n\n/**\n * Equality comparaison between 2 Uint8Arrays. Arrays are equal if they have the\n * same length and if all their components are equal to their counterpart\n * components at the same index.\n * @param a\n * @param b\n * @returns true if equal, false otherwise\n */\nexport function areUint8ArrayEqual(a: Uint8Array, b: Uint8Array): boolean {\n  if (a.length !== b.length) return false\n  for (let i = 0; i < a.length; i++) {\n    if (a[i] !== b[i]) return false\n  }\n  return true\n}\n","import { DEFAULT_CHUNK_SIZE } from \"@/config\"\nimport { FileChunk } from \"@/types\"\nimport { BytesCopiedFrom, keccak } from \"@/utils\"\n\n/**\n * Splits the content of a file into multiple chunks of the same size (except\n * if the remaining bytes of the last chunk don't cover a full chunk, in which\n * case a smaller chunk upload will be required). Chunks are also hashed, as\n * such this function returns tuples of (chunk, chunkHash).\n * @param content Raw byte content of the file\n * @param chunkSize Size of the chunks, it's recommend to pick the highest\n * possible chunk size for the targetted blockchain as to optimise the number\n * of write operations which will be performed. Depending on the use-case there\n * might be a need to create smaller chunks to allow for redundancy of similar\n * chunks uploaded to kick-in, resulting in write optimisations. As a reminder,\n * 32 bytes will be used to address a chunk in the store, as such every chunk\n * to be stored requires 32 bytes of extra storage.\n * @returns a list of chunks which can be uploaded to reconstruct the file\n */\nexport function chunkBytes(\n  content: Uint8Array,\n  chunkSize: number = DEFAULT_CHUNK_SIZE\n): FileChunk[] {\n  if (chunkSize == 0) {\n    throw new Error(`invalid chunk size, must be positive integer`)\n  }\n  const L = content.length\n  const nb = Math.ceil(L / chunkSize)\n  const chunks: FileChunk[] = []\n  let chunk: Uint8Array\n  for (let i = 0; i < nb; i++) {\n    chunk = BytesCopiedFrom(\n      content,\n      i * chunkSize,\n      Math.min(chunkSize, L - i * chunkSize)\n    )\n    chunks.push({\n      bytes: chunk,\n      hash: keccak(chunk),\n    })\n  }\n  return chunks\n}\n","import { INode } from \"@/types\"\n\nexport type InscriptionChunk<DataEncoding = Uint8Array> = {\n  type: \"chunk\"\n  content: DataEncoding\n}\n\nexport type InscriptionFile<DataEncoding = Uint8Array> = {\n  type: \"file\"\n  metadata: DataEncoding[]\n  chunks: DataEncoding[]\n}\n\nexport type InscriptionDirectory<DataEncoding = Uint8Array> = {\n  type: \"directory\"\n  files: {\n    [name: string]: DataEncoding\n  }\n}\n\nexport type Inscription<DataEncoding = Uint8Array> =\n  | InscriptionChunk<DataEncoding>\n  | InscriptionFile<DataEncoding>\n  | InscriptionDirectory<DataEncoding>\n\n/**\n * Traverse the inverted tree starting by the root, creating inscriptions as\n * it's being traversed. At the end of the flow the inscriptions will be\n * reversed to ensure they are written to the store in the right order (as the\n * onchfs will reject inodes pointing to inexisting resources; let it be file\n * chunks or directory files).\n * @param root The root of the tree, can be either the root directory or a file\n * @returns A list of inscription objects ready to be turned into operations\n */\nexport function generateInscriptions(root: INode): Inscription[] {\n  const inscriptions: Inscription[] = []\n  const traverse = (node: INode) => {\n    if (node.type === \"directory\") {\n      inscriptions.push({\n        type: \"directory\",\n        files: Object.fromEntries(\n          Object.keys(node.files).map(name => [name, node.files[name].cid])\n        ),\n      })\n      // recursively traverse each inode of the directory\n      for (const name in node.files) {\n        traverse(node.files[name])\n      }\n    } else if (node.type === \"file\") {\n      // create the file inscription first as it will be reversed in the end,\n      // so the chunk inscriptions will appear first\n      inscriptions.push({\n        type: \"file\",\n        chunks: node.chunks.map(chk => chk.hash),\n        metadata: node.metadata,\n      })\n      for (const chunk of node.chunks) {\n        inscriptions.push({\n          type: \"chunk\",\n          content: chunk.bytes,\n        })\n      }\n    }\n  }\n  traverse(root)\n  return inscriptions.reverse()\n}\n\n/**\n * Compute the number of bytes an inscription will take on the storage.\n * @param ins Inscription for which storage space should be computed\n * @returns The number of storage bytes the inscription will take\n */\nfunction inscriptionStorageBytes(ins: Inscription) {\n  switch (ins.type) {\n    case \"chunk\":\n      // chunk to write + chunk key\n      return ins.content.byteLength + 32\n    case \"directory\":\n      // for every file in directory:\n      //  - 32 bytes for pointer\n      //  - 1 byte per character (stored in 7-bit ASCII)\n      // and 32 bytes for the directory pointer\n      return (\n        Object.keys(ins.files)\n          .map(name => name.length + 32)\n          .reduce((a, b) => a + b, 0) + 32\n      )\n    case \"file\":\n      // 32 bytes per chunk + metadata + 32 bytes for pointer\n      return (\n        32 + // 32 bytes for pointer\n        32 * ins.chunks.length + // 32 bytes per chunk\n        ins.metadata.map(buf => buf.byteLength).reduce((a, b) => a + b, 0)\n      )\n  }\n}\n\n/**\n * Computes the maximum number of storage bytes which will be consumed by the\n * inscriptions when they are written on-chain. This is a maximum value, as\n * some chunks/files/directories may already have been written to the storage.\n * Note: this doesn't include eventual gas execution fees, which are blockchain-\n * dependant.\n * @param inscriptions Inscriptions for which storage bytes will be computed\n * @returns Number of bytes the inscriptions may take on the storage\n */\nexport function inscriptionsStorageBytes(inscriptions: Inscription[]) {\n  return inscriptions.reduce(\n    (acc, ins) => inscriptionStorageBytes(ins) + acc,\n    0\n  )\n}\n","import { gzip } from \"pako\"\nimport { DEFAULT_CHUNK_SIZE, INODE_BYTE_IDENTIFIER } from \"@/config\"\nimport { FileMetadataEntries, encodeFileMetadata } from \"./metadata\"\nimport { FileInode } from \"@/types\"\nimport { lookup as lookupMime } from \"mime-types\"\nimport { chunkBytes } from \"./chunks\"\nimport { concatUint8Arrays, keccak } from \"@/utils\"\n// import { fileTypeFromBuffer } from \"file-type\"\n\n/**\n * Computes all the necessary data for the inscription of the file on-chain.\n * Performs the following tasks in order:\n *  - infer/detect the mime-type from the filename/content\n *  - compress the content in gzip using zopfli, but only use the compressed\n *    bytes if they are smaller in size than the raw content\n *  - build the metadata object from previous steps, and encode the metadata in\n *    the format supported by the blockchain\n *  - chunk the content of the file\n *  - compute the CID of the file based on its content & its metadata\n * @param name The filename, will only be used to infer the Mime Type (a magic number cannot be used for text files so this is the privileged method)\n * @param content Byte content of the file, as a Buffer\n * @param chunkSize Max number of bytes for chunking the file content\n * @returns A file node object with all the data necessary for its insertion\n */\nexport async function prepareFile(\n  name: string,\n  content: Uint8Array,\n  chunkSize: number = DEFAULT_CHUNK_SIZE\n): Promise<FileInode> {\n  let metadata: FileMetadataEntries = {}\n  let insertionBytes = content\n  // we use file extension to get mime type\n  let mime = lookupMime(name)\n  // if no mime type can be mapped from filename, use magic number\n  if (!mime) {\n    // const magicMime = await fileTypeFromBuffer(content)\n    // if (magicMime) {\n    //   metadata[\"Content-Type\"] = magicMime.mime\n    // }\n    // if still no mime, we simply do not set the Content-Type in the metadata,\n    // and let the browser handle it.\n    // We could set it to \"application/octet-stream\" as RFC2046 states, however\n    // we'd be storing this whole string on-chain for something that's probably\n    // going to be inferred as such in any case;\n  } else {\n    metadata[\"Content-Type\"] = mime\n  }\n\n  // compress into gzip using node zopfli, only keep if better\n  const compressed = gzip(content)\n  if (compressed.byteLength < insertionBytes.byteLength) {\n    insertionBytes = compressed\n    metadata[\"Content-Encoding\"] = \"gzip\"\n  }\n\n  // chunk the file\n  const chunks = chunkBytes(insertionBytes, chunkSize)\n  // encode the metadata\n  const metadataEncoded = encodeFileMetadata(metadata)\n  // compute the file unique identifier, following the onchfs specifications:\n  // keccak( 0x01 , keccak( content ), keccak( metadata ) )\n  const contentHash = keccak(insertionBytes)\n  const metadataHash = keccak(concatUint8Arrays(...metadataEncoded))\n  const cid = keccak(\n    concatUint8Arrays(INODE_BYTE_IDENTIFIER.FILE, contentHash, metadataHash)\n  )\n\n  return {\n    type: \"file\",\n    cid,\n    chunks,\n    metadata: metadataEncoded,\n  }\n}\n","import {\n  BytesCopiedFrom,\n  areUint8ArrayEqual,\n  compareUint8Arrays,\n  concatUint8Arrays,\n} from \"@/utils\"\n\nexport interface FileMetadataEntries {\n  \"Content-Type\"?: string\n  \"Content-Encoding\"?: \"gzip\" | \"deflate\" | \"compress\"\n}\n\nexport type FileMetadataBytecodes = {\n  [entry in keyof FileMetadataEntries]: Uint8Array\n}\n\n// map of the metadata fields with their 2-byte identifier, used to encode\n// on the blockchain with a smaller footprint\nexport const fileMetadataBytecodes: FileMetadataBytecodes = {\n  \"Content-Type\": new Uint8Array([0, 1]),\n  \"Content-Encoding\": new Uint8Array([0, 2]),\n}\n\n// a list of the forbidden characters in the metadata\n// todo: point to where I found this in http specs\nexport const FORBIDDEN_METADATA_CHARS = [\n  0, // NUL character\n]\n\n/**\n * Validate a metadata field value to check if if follows https contrasts.\n * todo: should be refined to properly implement the HTTP spec, right now just\n *       NUL is verified\n * @param value The metadata field value\n */\nexport function validateMetadataValue(value: string): void {\n  for (let i = 0; i < value.length; i++) {\n    if (FORBIDDEN_METADATA_CHARS.includes(value.charCodeAt(i))) {\n      throw new Error(\n        `contains invalid character (code: ${value.charCodeAt(\n          i\n        )}) at position ${i}`\n      )\n    }\n  }\n}\n\n/**\n * Encodes the metadata of a file following the specifications provided by the\n * onchfs. Each entry is prefixed by 2 bytes encoding the entry type, followed\n * by 7-bit ASCII encoded characters for the string-value associated.\n * The metadata entries are sorted by their 2 bytes identifier.\n * @param metadata The object metadata of a file\n * @returns An array of buffers, each entry representing one metadata property\n */\nexport function encodeFileMetadata(\n  metadata: FileMetadataEntries\n): Uint8Array[] {\n  const out: Uint8Array[] = []\n  let value: string\n  for (const entry in metadata) {\n    if (fileMetadataBytecodes[entry]) {\n      // only process if valid entry\n      value = metadata[entry]\n      try {\n        validateMetadataValue(value)\n      } catch (err: any) {\n        throw new Error(\n          `Error when validating the metadata field \"${entry}\": ${err.message}`\n        )\n      }\n      out.push(\n        concatUint8Arrays(\n          fileMetadataBytecodes[entry],\n          new TextEncoder().encode(value)\n        )\n      )\n    }\n  }\n  out.sort(compareUint8Arrays)\n  return out\n}\n\nexport function decodeMetadataEntry(entry: Uint8Array) {\n  const bytecodeEntry = BytesCopiedFrom(entry, 0, 2)\n  for (const [header, bytecode] of Object.entries(fileMetadataBytecodes)) {\n    if (areUint8ArrayEqual(bytecodeEntry, bytecode)) {\n      return [header, new TextDecoder().decode(BytesCopiedFrom(entry, 2))]\n    }\n  }\n  return null\n}\n\nexport function decodeFileMetadata(raw: Uint8Array[]) {\n  const metadata: FileMetadataEntries = {}\n  for (const line of raw) {\n    const entry = decodeMetadataEntry(line)\n    if (entry) {\n      metadata[entry[0]] = entry[1]\n    }\n  }\n  return metadata\n}\n","import { DEFAULT_CHUNK_SIZE, INODE_BYTE_IDENTIFIER } from \"../config\"\nimport { prepareFile } from \"@/files/file\"\nimport {\n  DirectoryInode,\n  IFile,\n  INode,\n  PrepareDirectoryDir,\n  PrepareDirectoryFile,\n  PrepareDirectoryNode,\n} from \"@/types\"\nimport { concatUint8Arrays, keccak } from \"@/utils\"\n\n/**\n * Encodes the filename in 7-bit ASCII, where UTF-8 characters are escaped. Will\n * also escape any character that are not supported in the URI specification, as\n * these will be fetched using a similar pattern by browsers. The native\n * `encodeURIComponent()` method will be used for such a purpose.\n * @param name Filename to encode\n * @returns Filename encoded in 7-bit ASCII\n */\nexport function encodeFilename(name: string): string {\n  return encodeURIComponent(name)\n}\n\n/**\n * Computed the different component of a directory inode based on the\n * preparation object.\n * @param dir A directory being prepared\n * @returns A directory inode, from which insertions can be derived\n */\nexport function computeDirectoryInode(\n  dir: PrepareDirectoryDir\n): DirectoryInode {\n  const acc: Uint8Array[] = []\n  const filenames = Object.keys(dir.files).sort()\n  const dirFiles: Record<string, INode> = {}\n  for (const filename of filenames) {\n    const inode = dir.files[filename].inode!\n    dirFiles[filename] = inode\n    // push filename hashed\n    acc.unshift(keccak(filename))\n    // push target inode cid\n    acc.unshift(inode.cid)\n  }\n  // add indentifying byte at the beginning\n  acc.unshift(INODE_BYTE_IDENTIFIER.DIRECTORY)\n  return {\n    type: \"directory\",\n    cid: keccak(concatUint8Arrays(...acc)),\n    files: dirFiles,\n  }\n}\n\n/**\n * Builds a graph from a list of files (relative path from the directory root,\n * content) in a folder structure as it's going to be inscribed on the file\n * system.\n * @param files A list of the files (& their content), where paths are specified with separating \"/\"\n * @returns A tuple of (graph, leaves), where graph is a structure ready to be\n * parsed for insertion & leaves the leaves of the graph, entry points for\n * parsing the graph in reverse.\n */\nexport function buildDirectoryGraph(\n  files: IFile[]\n): [PrepareDirectoryDir, PrepareDirectoryFile[]] {\n  let graph: PrepareDirectoryDir = {\n    type: \"directory\",\n    files: {},\n    parent: null,\n  }\n  const leaves: PrepareDirectoryFile[] = []\n\n  for (const file of files) {\n    let active = graph,\n      part: string = \"\"\n    const formattedPath = file.path.startsWith(\"./\")\n      ? file.path.slice(2)\n      : file.path\n    // note: the filenames get encoded here, as the onchfs spec defines\n    // filenames need to be inserted in ASCII 7-bit with special characters\n    // escape-encoded\n    const parts = formattedPath.split(\"/\").map(part => encodeFilename(part))\n    for (let i = 0; i < parts.length; i++) {\n      part = parts[i]\n      // if name is empty, we throw\n      if (part.length === 0) {\n        throw new Error(\n          `The file ${file.path} contains an invalid part, there must be at least 1 character for each part.`\n        )\n      }\n      // if it's the last part, store it as a file\n      if (i === parts.length - 1) {\n        // if the leaf already exists, we throw an error: there cannot be 2\n        // nodes identified by the same path\n        if (active.files.hasOwnProperty(part)) {\n          throw new Error(\n            `The file at path ${file.path} is colliding with another path in the directory. There mush be a single path pointing to a file.`\n          )\n        }\n        const nLeave: PrepareDirectoryFile = {\n          type: \"file\",\n          content: file.content,\n          name: part,\n          parent: active,\n        }\n        active.files[part] = nLeave\n        leaves.push(nLeave)\n      }\n      // it's a directory, so we need to navigate to it or create a new one\n      else {\n        if (active.files.hasOwnProperty(part)) {\n          active = active.files[part] as any\n        } else {\n          const nDir: PrepareDirectoryDir = {\n            type: \"directory\",\n            files: {},\n            parent: active,\n          }\n          active.files[part] = nDir\n          active = nDir\n        }\n      }\n    }\n  }\n\n  return [graph, leaves]\n}\n\n/**\n * Given a list of files, will create an inverted tree of the directory\n * structure with the main directory as its root. Each file will be chunked in\n * preparation for the insertion. The whole structure will be ready for\n * computing the inscriptions on any blockchain network on which the protocol\n * is deployed.\n * @param files A list a files (with their path relative to the root of the\n * directory and their byte content)\n * @param chunkSize Maximum size of the chunks in which the file will be divided\n * @returns A root directory inode from which the whole directory tree can be\n * traversed, as it's going to be inscribed.\n */\nexport async function prepareDirectory(\n  files: IFile[],\n  chunkSize: number = DEFAULT_CHUNK_SIZE\n): Promise<DirectoryInode> {\n  const [graph, leaves] = buildDirectoryGraph(files)\n\n  const parsed: PrepareDirectoryNode[] = []\n  let parsing: PrepareDirectoryNode[] = leaves\n\n  while (parsing.length > 0) {\n    const nextParse: PrepareDirectoryNode[] = []\n    for (const node of parsing) {\n      // if this node has already been parsed, ignore\n      if (parsed.includes(node)) continue\n      if (node.type === \"file\") {\n        node.inode = await prepareFile(node.name, node.content, chunkSize)\n      } else if (node.type === \"directory\") {\n        // compute the inode associated with the directory\n        node.inode = computeDirectoryInode(node)\n      }\n      // marked the node as parsed\n      parsed.push(node)\n      // push the eventual parent to the nodes to parse; eventually when\n      // reaching the head, nothing will have to get parsed\n      if (node.parent) {\n        // we can only push the parent when all its children have been parsed\n        // already (which is checked if .inode property exists)\n        const children = Object.values(node.parent.files)\n        if (!children.find(child => !child.inode)) {\n          nextParse.push(node.parent)\n        }\n      }\n    }\n    // once all the nodes to parse have been parsed, assign the next wave\n    parsing = nextParse\n  }\n\n  // at this point graph.inodes has been populated with the root directory node,\n  // which happens to be linked to the rest of the inodes; it can returned\n  return graph.inode!\n}\n","/**\n * List of the blockchain supported officially. While the protocol can be\n * deployed anywhere, the URI resolution is more easily inferred from the\n * supported deployments.\n */\nexport const blockchainNames = [\"tezos\", \"ethereum\"] as const\nexport type BlockchainNames = typeof blockchainNames[number]\n\n/**\n * The URI Authority defines the \"host\" of an asset, in this case a combination\n * of a blockchain (represented by string indentifier & chain ID) & a contract\n * on such blockchain compliant to the onchfs specifications.\n */\nexport interface URIAuthority {\n  blockchainName: string\n  contract: string\n  blockchainId: string\n}\n\n/**\n * Provides a full broken-down representation of the URI in a formatted way.\n * While the authority is an optionnal segment when writing an onchfs URI, it\n * must be inferred during the resolution by providing a context.\n */\nexport interface URIComponents {\n  cid: string\n  authority: URIAuthority\n  path?: string\n  query?: string\n  fragment?: string\n}\n\n/**\n * Some URI Context needs to be provided for most URI resolutions, as often\n * onchfs URI will rely of the context in which they are stored/seen to infer\n * their resolution. For instance, a base onchfs URI stored on ETH mainnet will\n * expect solvers to point to the main onchfs ETH contract.\n */\nexport type URIContext = Pick<URIAuthority, \"blockchainName\"> &\n  Pick<Partial<URIAuthority>, \"blockchainId\" | \"contract\">\n\n/**\n * TODO: insert true values here.\n * A naive map of the \"official\" onchfs Smart Contracts.\n */\nexport const defaultContractsMap: Record<string, string> = {\n  \"tezos:mainnet\": \"KT1WvzYHCNBvDSdwafTHv7nJ1dWmZ8GCYuuC\",\n  \"tezos:ghostnet\": \"KT1XZ2FyRNtzYCBoy18Rp7R9oejvFSPqkBoy\",\n  \"ethereum:1\": \"b0e58801d1b4d69179b7bc23fe54a37cee999b09\",\n  \"ethereum:5\": \"fcfdfa971803e1cc201f80d8e74de71fddea6551\",\n}\n\n/**\n * Proper charsets tightly following the spec\n */\n\nconst LOW_ALPHA = \"a-z\"\nconst HI_ALPHA = \"A-Z\"\nconst ALPHA = LOW_ALPHA + HI_ALPHA\nconst DIGIT = \"0-9\"\nconst SAFE = \"$\\\\-_.+\"\nconst EXTRA = \"!*'(),~\"\nconst HEX_CHARSET = \"A-Fa-f0-9\"\nconst LOW_RESERVED = \";:@&=\"\nconst RESERVED = LOW_RESERVED + \"\\\\/?#\"\nconst UNRESERVED = ALPHA + DIGIT + SAFE + EXTRA\nconst PCT_ENCODED = `%[${HEX_CHARSET}]{2}`\nconst UCHAR = `(?:(?:[${UNRESERVED}])|(?:${PCT_ENCODED}))`\nconst XCHAR = `(?:(?:[${UNRESERVED}${RESERVED}])|(?:${PCT_ENCODED}))`\n\nconst URI_CHARSET = XCHAR\nconst B58_CHARSET = \"1-9A-HJ-NP-Za-km-z\"\nconst AUTHORITY_CHARSET = `${HEX_CHARSET}${B58_CHARSET}.a-z:`\nconst SEG_CHARSET = `(?:(?:${UCHAR})|[${LOW_RESERVED}])`\nconst QUERY_CHARSET = `(?:${SEG_CHARSET}|\\\\/|\\\\?)`\n\n/**\n * Parses an absolute onchfs URI, following its ABNF specification. If any part\n * of the URI is mal-constructed, or if some context is missing to fully\n * resolve it, this function will throw with an error indicating where the\n * resolution has failed.\n *\n * @dev The resolution happens in 3 distinctive steps, each breaking down the\n * URI into smaller components which can be parsed more easily:\n *\n * * parseSchema\n *   <onchfs>://<schema-specific-part>\n *   Splits the URI into it's 2 biggest sections, extracting the schema from\n *   the schema-specific part.\n * * parseSchemaSpecificPart\n *   [ authority \"/\" ] cid [path] [ \"?\" query ] [ \"#\" fragment ]\n *   Splits the schema-specific part into its various logical segments. This\n *   step validate the general structure of the URI as well.\n * * parseAuthority\n *   [ contract-address \".\" ] blockchain-name [ \":\" chainid ]\n *   The authority segment is parsed, and using the context provided by the\n *   consumer it tries to resolve the authority.\n *\n * @param uri An absolute onchfs URI to be parsed\n * @param context The context in which the URI has been observed; such context\n * may be required for resolving the URI as it might be written in its short\n * form to save storage space, relying on inference from context; which is\n * valid part of the spec designed to optimise storage costs.\n * @returns A fully formed URI Components object, describing the various URI\n * components.\n */\nexport function parseURI(uri: string, context?: URIContext): URIComponents {\n  try {\n    // pass the first layer: validates the high level format of the URI:\n    // onchfs://<schema-specific-part>\n    // and returns the schema-specific part if the URI has a general valid\n    // format.\n    const schemaSpecificPart = parseSchema(uri)\n    // parse the schema-specific part\n    // [ authority \"/\" ] cid [path] [ \"?\" query ] [ \"#\" fragment ]\n    const schemaSegments = parseSchemaSpecificPart(schemaSpecificPart)\n    // parse the authority\n    const authority = parseAuthority(schemaSegments.authority, context)\n\n    return {\n      ...schemaSegments,\n      cid: schemaSegments.cid.toLowerCase(),\n      authority: authority,\n    }\n  } catch (err: any) {\n    // catch to prefix low-level error message with generic message\n    throw new Error(\n      `Error when parsing the URI \"${uri}\" as a onchfs URI: ${err.message}`\n    )\n  }\n}\n\n/**\n * 1st order URI parsing; checks if the overall URI is valid by looking at the\n * protocol, and the schema-specific part. If any character in the URI invalid\n * (not part of the allowed URI characters), will throw.\n * Will also thrown if the general pattern doesn't not comply with onchfs URIs.\n * @param uri The URI to parse\n * @returns The URI schema-specific part\n */\nexport function parseSchema(uri: string): string {\n  const regex = new RegExp(`^(onchfs):\\/\\/(${URI_CHARSET}{64,})$`)\n  const results = regex.exec(uri)\n\n  // result is null, the regex missed\n  if (!results) {\n    throw new Error(\n      `general onchfs URI format is invalid / Pattern didn't match: ${regex.toString()}`\n    )\n  }\n\n  return results[2]\n}\n\n/**\n * The different segments of the URI Schema-Specific Component:\n * [ authority \"/\" ] cid [path] [ \"?\" query ] [ \"#\" fragment ]\n */\nexport interface URISchemaSpecificParts {\n  cid: string\n  authority?: string\n  path?: string\n  query?: string\n  fragment?: string\n}\n\n/**\n * Parses the schema-specific component (onchfs://<schema-specific-component>)\n * into a list of sub-segments based on the onchfs URI specification.\n * [ authority \"/\" ] cid [path] [ \"?\" query ] [ \"#\" fragment ]\n * @param uriPart THe URI Schema-Specific Component\n * @returns An object with the different segments isolated\n */\nexport function parseSchemaSpecificPart(\n  uriPart: string\n): URISchemaSpecificParts {\n  const authorityReg = `([${AUTHORITY_CHARSET}]*)\\\\/`\n  const cidReg = `[${HEX_CHARSET}]{64}`\n  const pathReg = `${SEG_CHARSET}*(?:\\\\/${SEG_CHARSET}*)*`\n  const queryReg = `\\\\?(${QUERY_CHARSET}*)`\n  const fragReg = `#(${QUERY_CHARSET}*)`\n\n  // isolates each segment of the URI based on their pattern, including\n  // cardinality of every segment\n  const regex = new RegExp(\n    `^(?:${authorityReg})?(${cidReg})(?:\\\\/(${pathReg}))?(?:${queryReg})?(?:${fragReg})?$`\n  )\n\n  const res = regex.exec(uriPart)\n\n  if (!res) {\n    throw new Error(\n      `the URI schema specific component seems to be invalid. \"${uriPart}\" should respect the following pattern: ${regex.toString()}`\n    )\n  }\n\n  const [_, authority, cid, path, query, fragment] = res\n\n  return {\n    authority,\n    cid,\n    path,\n    query,\n    fragment,\n  }\n}\n\nconst blockchainAuthorityParsers: Record<BlockchainNames, () => RegExp> = {\n  tezos: () =>\n    new RegExp(\n      `^(?:(KT(?:1|2|3|4)[${B58_CHARSET}]{33})\\\\.)?(tezos|tez|xtz)(?::(ghostnet|mainnet))?$`\n    ),\n  ethereum: () =>\n    new RegExp(`^(?:([${HEX_CHARSET}]{40})\\\\.)?(ethereum|eth)(?::([0-9]+))?$`),\n}\n\ntype BlockchainNameVariants = {\n  [K in BlockchainNames]: [K, ...string[]]\n}\nconst blockchainNameVariants: BlockchainNameVariants = {\n  tezos: [\"tezos\", \"tez\", \"xtz\"],\n  ethereum: [\"ethereum\", \"eth\"],\n}\n\ntype BlockchainDefaultNetwork = {\n  [K in BlockchainNames]: string\n}\nconst blockchainDefaultNetwork: BlockchainDefaultNetwork = {\n  tezos: \"mainnet\",\n  ethereum: \"1\",\n}\n\n/**\n * Given the string segment of the authority (or lack thereof) and a context in\n * which the URI exists, resolves the authority object (blockchain, chainid,\n * smart contract address) in which the object is supposed to be stored.\n *\n * The resolution is initialized with the provided context, after which the\n * authority segment of the URI (onchfs://<authority>/<cid>/<path>...) is parsed\n * and eventually overrides the given context (as some resources living in a\n * given context are allowed to reference assets existing in other contexts).\n *\n * If some authority components are still missing after the parsing, the\n * blockchain name is used to infer (chainid, contract address). In case\n * only contract is missing, it is inferred from (blockchain name, chainid).\n *\n * If any component is missing at the end of this process (ie: cannot be found\n * in the context, URI, and cannot be inferred), this functions throws.\n *\n * @param authority The string segment of the authority in the URI. If the\n * authority is missing from the CID, a context must be provided to resolve\n * the authority component.\n * @param context The context in which the authority is loaded. If such context\n * is not provided, the authority segment must have a blockchain name so that\n * the authority can be resolved using defaults.\n * @returns An object containing all the segments of the authority. If segment\n * doesn't exist, the context provided will be used to infer all the authority\n * components.\n */\nexport function parseAuthority(\n  authority?: string,\n  context?: URIContext\n): URIAuthority {\n  // initialise the authority object to the given context\n  let tmp: Partial<URIAuthority> = { ...context }\n\n  if (authority) {\n    // loop through every blockchain and use its authority-regex to identify\n    // the different parts, potentially\n    let regex: RegExp, res: RegExpExecArray | null\n    for (const name of blockchainNames) {\n      // generate the blockchain-related regex and parse the authority\n      regex = blockchainAuthorityParsers[name]()\n      res = regex.exec(authority)\n      // no result; move to next blockchain\n      if (!res) continue\n      // results are in slots [1;3] - assign to temp object being parsed\n      const [contract, blockchainName, blockchainId] = res.splice(1, 3)\n      contract && (tmp.contract = contract)\n      blockchainName && (tmp.blockchainName = blockchainName)\n      blockchainId && (tmp.blockchainId = blockchainId)\n      break\n    }\n  }\n\n  // at this stage, if there isn't a blockchain name, then we can throw as the\n  // network is missing (either from the URI or the context)\n  if (!tmp.blockchainName) {\n    throw new Error(\n      \"the blockchain could not be inferred when parsing the URI, if the URI doesn't have an authority segment (onchfs://<authority>/<cid>/<path>...), a context should be provided based on where the URI was observed. The blockchain needs to be resolved either through the URI or using the context.\"\n    )\n  }\n  // normalize blockchain name into its cleanest and most comprehensible form\n  for (const [name, values] of Object.entries(blockchainNameVariants)) {\n    if (values.includes(tmp.blockchainName)) {\n      tmp.blockchainName = name\n      break\n    }\n  }\n\n  // if blockchain ID is missing, then assign the default blockchain ID\n  // associated with the asset, which is mainnet\n  if (!tmp.blockchainId) {\n    tmp.blockchainId = blockchainDefaultNetwork[tmp.blockchainName]\n  }\n  // if no blockchain ID at this stage, the blockchain name cannot be mapped\n  // to a given blockchain ID\n  if (!tmp.blockchainId) {\n    throw new Error(\n      `The blockchain identifier could not be inferred from the blockchain name when parsing the authority segment of the URI. This can happen when a blockchain not supported by the onchfs package was provided in the context of the URI resolution, yet a blockchain ID wasn't provided in the context nor could it be found in the URI.`\n    )\n  }\n\n  // if contract is missing, default one for blockchain (name, id) is picked\n  if (!tmp.contract) {\n    tmp.contract =\n      defaultContractsMap[`${tmp.blockchainName}:${tmp.blockchainId}`]\n  }\n  // if no contract at this stage, it's mostly because some unofficial context\n  // has been forced, yet no contract was given in the context\n  if (!tmp.contract) {\n    throw new Error(\n      `A File Object contract could not be associated with the onchfs URI. This can happen when an unsupported blockchain was provided as a context to the URI resolver, yet no contract was provided in the context, nor could it be parsed from the URI. The URI must resolve to a Smart Contract.`\n    )\n  }\n\n  return tmp as URIAuthority\n}\n\n/**\n * Serializes an URI for a given context. The more context is associated with\n * the URI, the more its <authority> segment can be inferred by parsers.\n * @param components The various URI components\n * @param context The context in which the URI will be observed\n * @returns The serialized URI\n */\n// export function serializeURI(\n//   components: URIComponents,\n//   context?: URIContext\n// ): string {\n//   let uri = \"onchfs://\"\n//   if (context?.blockchainName)\n// }\n\n// ctx: tezos|ghostnet\n\n// export function normalizeURIComponents(components: URIComponents)\n","import { ProxyResolutionStatusErrors } from \"./proxy\"\n\n/**\n * Error thrown during the resolution of a relative URI by the proxy.\n */\nexport class OnchfsProxyResolutionError extends Error {\n  constructor(message: string, public status: ProxyResolutionStatusErrors) {\n    super(message)\n    this.name = \"OnchfsProxyResolutionError\"\n  }\n}\n","import { FileMetadataEntries, decodeFileMetadata } from \"@/files/metadata\"\nimport {\n  URIAuthority,\n  URISchemaSpecificParts,\n  parseAuthority,\n  parseSchema,\n  parseSchemaSpecificPart,\n} from \"./uri\"\nimport { hexStringToBytes } from \"@/utils\"\nimport { OnchfsProxyResolutionError } from \"./errors\"\n\ninterface InodeFileNativeFS {\n  cid: string\n  metadata: (string | Uint8Array)[]\n  chunkPointers: string[]\n}\n\ninterface InodeDirectoryNativeFS {\n  cid: string\n  files: Record<string, string>\n}\n\nexport type InodeNativeFS = InodeFileNativeFS | InodeDirectoryNativeFS\n\nexport interface Resolver {\n  getInodeAtPath: (\n    cid: string,\n    path: string[],\n    authority?: URIAuthority\n  ) => Promise<InodeNativeFS | null>\n  readFile: (\n    cid: string,\n    chunkPointers: string[],\n    authority?: URIAuthority\n  ) => Promise<string | Uint8Array>\n}\n\nexport enum ProxyResolutionStatusErrors {\n  NOT_ACCEPTABLE = 406,\n  NOT_FOUND = 404,\n  BAD_REQUEST = 400,\n  INTERNAL_SERVER_ERROR = 500,\n}\n\nconst ResolutionErrors: Record<ProxyResolutionStatusErrors, string> = {\n  [ProxyResolutionStatusErrors.BAD_REQUEST]: \"Bad Request\",\n  [ProxyResolutionStatusErrors.NOT_ACCEPTABLE]: \"Resource Cannot be Served\",\n  [ProxyResolutionStatusErrors.NOT_FOUND]: \"Not Found\",\n  [ProxyResolutionStatusErrors.INTERNAL_SERVER_ERROR]: \"Internal Server Error\",\n}\n\nexport enum ProxyResolutionStatusSuccess {\n  OK = 200,\n}\n\nexport enum ProxyResolutionStatusRedirect {\n  PERMANENT = 308,\n}\n\nexport type ProxyResolutionStatus =\n  | ProxyResolutionStatusSuccess\n  | ProxyResolutionStatusErrors\n  | ProxyResolutionStatusRedirect\n\nexport interface ProxyResolutionError {\n  code: number\n  name: string\n  message?: string\n}\n\ninterface ProxyExtraHeaders {\n  Location: string\n}\n\ntype ProxyHttpHeaders = FileMetadataEntries | ProxyExtraHeaders\n\nexport interface ProxyResolutionResponse {\n  status: ProxyResolutionStatus\n  content: Uint8Array\n  headers: ProxyHttpHeaders\n  error?: ProxyResolutionError\n}\n\n/**\n * Creates a generic-purpose URI resolver, leaving the implementation of\n * the file system resource resolution to the consumer of this API. The\n * resolver is a function of URI, which executes a series of file system\n * operations based on the URI content and the responses from the file\n * system.\n *\n * @param resolver An object which implements low-level retrieval methods to\n * fetch the necessary content from the file system in order to resolve the\n * URI.\n * @returns A function which takes an URI as an input and executes a serie of\n * operations to retrieve the file targetted by the URI. The resolution\n * of the file pointers against the file system is left to the consuming\n * application, which can implement different strategies based on the\n * use-cases.\n */\nexport function createProxyResolver(resolver: Resolver) {\n  /**\n   * A function which takes an URI as an input and executes a serie of\n   * operations to retrieve the file targetted by the URI. The resolution\n   * of the file pointers against the file system is left to the consuming\n   * application, which can implement different strategies based on the\n   * use-cases.\n   * @param uri An URI (schema-prefixed or not) pointing to the asset.\n   */\n  return async (uri: string): Promise<ProxyResolutionResponse> => {\n    try {\n      if (uri.startsWith(\"/\")) {\n        uri = uri.slice(1)\n      }\n      // extract the main components of interest: path & cid\n      let components: URISchemaSpecificParts\n      try {\n        // if for some reason an absolute URI was provided at the proxy-\n        // resolution level, extract it from the URI which will get parsed\n        if (uri.startsWith(\"onchfs://\")) {\n          uri = parseSchema(uri)\n        }\n        // only the schema-specific part is parsed\n        components = parseSchemaSpecificPart(uri)\n      } catch (err) {\n        throw new OnchfsProxyResolutionError(\n          `The onchfs URI is invalid: ${err.message}`,\n          ProxyResolutionStatusErrors.BAD_REQUEST\n        )\n      }\n\n      let { cid, path, authority } = components\n      // path segments are separated in array, empty segments are filtered out\n      let paths = path?.split(\"/\") || []\n      paths = paths.filter(pt => pt.length > 0)\n\n      // if there's an authority, parse it\n      let parsedAuthority: URIAuthority | undefined\n      if (authority) {\n        parsedAuthority = parseAuthority(authority)\n      }\n\n      // resolve the inode at the given target\n      let inode: InodeNativeFS\n      let mainInode: InodeNativeFS\n      try {\n        inode = mainInode = await resolver.getInodeAtPath(\n          cid,\n          paths,\n          parsedAuthority\n        )\n        if (!inode) {\n          throw new OnchfsProxyResolutionError(\n            `Could not find any inode at (${cid}, ${path})`,\n            ProxyResolutionStatusErrors.NOT_FOUND\n          )\n        }\n      } catch (err) {\n        // if the error is already an instance of ProxyResolutionError, simply\n        // forward it, otherwise craft a 500 error and throw it\n        if (err instanceof OnchfsProxyResolutionError) {\n          throw err\n        } else {\n          throw new OnchfsProxyResolutionError(\n            err.message,\n            ProxyResolutionStatusErrors.INTERNAL_SERVER_ERROR\n          )\n        }\n      }\n\n      // if the inode is a directory, try to get the index.html file\n      if ((inode as InodeDirectoryNativeFS).files) {\n        if ((inode as InodeDirectoryNativeFS).files[\"index.html\"]) {\n          try {\n            inode = await resolver.getInodeAtPath(\n              inode.cid,\n              [\"index.html\"],\n              parsedAuthority\n            )\n          } catch (err) {\n            // throw an internal server error\n            throw new OnchfsProxyResolutionError(\n              `An error occurred when resolving the index.html file inside the target directory at /${\n                inode.cid\n              }${err.message ? `: ${err.message}` : \"\"}`,\n              ProxyResolutionStatusErrors.INTERNAL_SERVER_ERROR\n            )\n          }\n        }\n        // if it's a directory but no index.html file at its root, throw\n        else {\n          throw new OnchfsProxyResolutionError(\n            `the inode at (${cid}, ${path}) is a directory which doesn't contain any index.html file, as such it cannot be served.`,\n            ProxyResolutionStatusErrors.NOT_ACCEPTABLE\n          )\n        }\n      }\n\n      // if the inode at this stage is a directory, throw\n      if ((inode as InodeDirectoryNativeFS).files || !inode) {\n        throw new OnchfsProxyResolutionError(\n          `could not find a file inode at (${cid}, ${path})`,\n          ProxyResolutionStatusErrors.NOT_FOUND\n        )\n      }\n\n      // fetch the file content (eventually normalize data to Uint8Array)\n      let content: Uint8Array\n      try {\n        const contentInput = await resolver.readFile(\n          inode.cid,\n          (inode as InodeFileNativeFS).chunkPointers,\n          parsedAuthority\n        )\n        content =\n          typeof contentInput === \"string\"\n            ? hexStringToBytes(contentInput)\n            : contentInput\n      } catch (err) {\n        throw new OnchfsProxyResolutionError(\n          `An error occurred when reading the content of the file of cid ${\n            inode.cid\n          }${err.message ? `: ${err.message}` : \"\"}`,\n          ProxyResolutionStatusErrors.INTERNAL_SERVER_ERROR\n        )\n      }\n\n      // parse the metadata from the file into an object header ready for an\n      // http response\n      let headers: ProxyHttpHeaders\n      const rawMetadataInput = (inode as InodeFileNativeFS).metadata\n      try {\n        const rawMetadata = rawMetadataInput.map(met =>\n          typeof met === \"string\" ? hexStringToBytes(met) : met\n        )\n        headers = decodeFileMetadata(rawMetadata)\n      } catch (err) {\n        throw new OnchfsProxyResolutionError(\n          `An error occurred when parsing the metadata of the file of cid ${\n            inode.cid\n          }, raw metadata bytes (${rawMetadataInput})${\n            err.message ? `: ${err.message}` : \"\"\n          }`,\n          ProxyResolutionStatusErrors.INTERNAL_SERVER_ERROR\n        )\n      }\n\n      let status: ProxyResolutionStatus = ProxyResolutionStatusSuccess.OK\n\n      // if the main inode is a directory, & the given path doesn't end with a\n      // trailing slash, we enforce a trailing slash at the end of the path to\n      // ensure that browsers relative URL resolution will resolve files\n      // relative to the one served starting at the end of the path.\n      //\n      // ex: /aeaeaeaeaeae   -> points to /aeaeaeaeaeae/index.html\n      // ->  /aeaeaeaeaeae/  -> redirect to\n      const wholeReqPath =\n        components.cid + (components.path ? `/${components.path}` : \"\")\n      if (\n        (mainInode as InodeDirectoryNativeFS).files &&\n        !wholeReqPath.endsWith(\"/\") &&\n        // in case a CID is followed by a \"/\" and an empty path, the \"/\" will\n        // disappear during parsing due to the URI specification, however an\n        // empty string path will appear, which is the only case where it\n        // appears; as such we can test it to cover this edge-case.\n        !(components.path === \"\")\n      ) {\n        const redirect =\n          \"/\" +\n          components.cid +\n          (components.path ? `/${components.path}` : \"\") +\n          \"/\" +\n          (components.query ? `?${components.query}` : \"\")\n        // add the Location header for redirect\n        headers = {\n          // preserve the existing headers; we will still be serving the content\n          ...headers,\n          Location: redirect,\n        }\n        status = ProxyResolutionStatusRedirect.PERMANENT\n      }\n\n      // everything is OK, the response can be served\n      return {\n        content,\n        headers,\n        status,\n      }\n    } catch (err) {\n      // an error was found; it's very likely that it's a ProxyResolutionError\n      // instance yet we procide a catch-all to ensure a 500 is returned if\n      // the errors happens to be something else, unexpected\n      let status: ProxyResolutionStatusErrors, error: ProxyResolutionError\n      if (err instanceof OnchfsProxyResolutionError) {\n        status = err.status\n        error = {\n          code: err.status,\n          name: ResolutionErrors[err.status],\n          message: err.message,\n        }\n      } else {\n        status = ProxyResolutionStatusErrors.INTERNAL_SERVER_ERROR\n        error = {\n          code: ProxyResolutionStatusErrors.INTERNAL_SERVER_ERROR,\n          name: ResolutionErrors[\n            ProxyResolutionStatusErrors.INTERNAL_SERVER_ERROR\n          ],\n        }\n      }\n      // forward the response\n      return {\n        status,\n        // we produce a basic html error page for proxy implementations which\n        // just want to forward the response\n        content: new TextEncoder().encode(\n          `<h1>${status} ${error.name}</h1>${\n            error.message ? `<p>${err.message}</p>` : \"\"\n          }`\n        ),\n        // same with headers\n        headers: {\n          \"Content-Type\": \"text/html; charset=utf-8\",\n        },\n        error,\n      }\n    }\n  }\n}\n","import * as chunks from \"./chunks\"\nimport * as dir from \"./directory\"\nimport * as file from \"./file\"\nimport * as inscriptions from \"./inscriptions\"\nimport * as metadata from \"./metadata\"\nimport { prepareFile } from \"./file\"\n\nexport default {\n  prepareFile,\n  prepareDirectory: dir.prepareDirectory,\n  generateInscriptions: inscriptions.generateInscriptions,\n  utils: {\n    chunkBytes: chunks.chunkBytes,\n    directory: {\n      encodeFilename: dir.encodeFilename,\n      computeInode: dir.computeDirectoryInode,\n      computeGraph: dir.buildDirectoryGraph,\n    },\n    inscriptions: {\n      computeStorageBytes: inscriptions.inscriptionsStorageBytes,\n    },\n    metadata: {\n      bytecodes: metadata.fileMetadataBytecodes,\n      validateValue: metadata.validateMetadataValue,\n      encode: metadata.encodeFileMetadata,\n      decode: metadata.decodeFileMetadata,\n    },\n  },\n}\n\nexport { prepareFile }\n","import { chunkBytes } from \"./files/chunks\"\nimport {\n  generateInscriptions,\n  inscriptionsStorageBytes,\n  Inscription,\n} from \"@/files/inscriptions\"\nimport {\n  buildDirectoryGraph,\n  computeDirectoryInode,\n  encodeFilename,\n  prepareDirectory,\n} from \"@/files/directory\"\nimport { prepareFile } from \"@/files/file\"\nimport { DEFAULT_CHUNK_SIZE, INODE_BYTE_IDENTIFIER } from \"@/config\"\nimport {\n  FORBIDDEN_METADATA_CHARS,\n  encodeFileMetadata,\n  fileMetadataBytecodes,\n  validateMetadataValue,\n} from \"@/files/metadata\"\nimport {\n  parseAuthority,\n  parseSchema,\n  parseSchemaSpecificPart,\n  parseURI,\n} from \"@/resolve/uri\"\nimport {\n  InodeNativeFS,\n  ProxyResolutionResponse,\n  createProxyResolver,\n} from \"./resolve/proxy\"\nimport { hexStringToBytes, keccak } from \"@/utils\"\nimport * as files from \"@/files\"\n\n/**\n * Wraps the low-level utility functions in a nested object to cleanup the\n * consumer API.\n */\nconst utils = {\n  keccak,\n  chunkBytes,\n  encodeFilename,\n  computeDirectoryInode,\n  buildDirectoryGraph,\n  validateMetadataValue,\n  encodeFileMetadata,\n  hexStringToBytes,\n}\n\n/**\n * Wraps the config variables in a nested object to cleanup the consumer API.\n */\nconst config = {\n  INODE_BYTE_IDENTIFIER,\n  DEFAULT_CHUNK_SIZE,\n  FORBIDDEN_METADATA_CHARS,\n  fileMetadataBytecodes,\n  inscriptionsStorageBytes,\n}\n\n/**\n * Wraps th URI-related variables in a nested object\n */\nconst uri = {\n  parse: parseURI,\n  utils: {\n    parseSchema,\n    parseSchemaSpecificPart,\n    parseAuthority,\n  },\n}\n\nconst resolver = {\n  create: createProxyResolver,\n}\n\nexport {\n  prepareFile,\n  prepareDirectory,\n  generateInscriptions,\n  utils,\n  config,\n  uri,\n  resolver,\n  files,\n}\n\nexport type { Inscription, InodeNativeFS, ProxyResolutionResponse }\n\nconst Onchfs = {\n  prepareFile,\n  prepareDirectory,\n  generateInscriptions,\n  utils,\n  config,\n  uri,\n  resolver,\n  files,\n}\nexport default Onchfs\n\n// Used to expose the library to the browser build version\nif (typeof window !== \"undefined\") {\n  ;(window as any).Onchfs = Onchfs\n}\n"]}